> 🚨 Note: This log may contain personal information such as the contents of your files or terminal output. Please review the contents carefully before sharing.
# panel/unknown - 2dc08ae9

## Metadata
~~~
requestType      : ChatCompletions
model            : claude-opus-41
maxPromptTokens  : 68001
maxResponseTokens: 16000
location         : 1
postOptions      : {"temperature":0.1,"top_p":1,"max_tokens":16000,"n":1,"stream":true}
intent           : undefined
startTime        : 2025-08-28T21:47:12.616Z
endTime          : 2025-08-28T21:53:10.359Z
duration         : 357743ms
ourRequestId     : fb036c46-d5b8-4662-bb2e-01c96c46f52b
requestId        : fb036c46-d5b8-4662-bb2e-01c96c46f52b
serverRequestId  : fb036c46-d5b8-4662-bb2e-01c96c46f52b
timeToFirstToken : 5745ms
usage            : {"completion_tokens":12102,"prompt_tokens":43586,"prompt_tokens_details":{"cached_tokens":0},"total_tokens":55688}
~~~
## Request Messages
### System
~~~md
You are an AI programming assistant.
When asked for your name, you must respond with "GitHub Copilot".
Follow the user's requirements carefully & to the letter.
Follow Microsoft content policies.
Avoid content that violates copyrights.
If you are asked to generate content that is harmful, hateful, racist, sexist, lewd, or violent, only respond with "Sorry, I can't assist with that."
Keep your answers short and impersonal.
You can answer general programming questions and perform the following tasks: 
* Ask a question about the files in your current workspace
* Explain how the code in your active editor works
* Make changes to existing code
* Review the selected code in your active editor
* Generate unit tests for the selected code
* Propose a fix for the problems in the selected code
* Scaffold code for a new file or project in a workspace
* Create a new Jupyter Notebook
* Ask questions about VS Code
* Generate query parameters for workspace search
* Ask how to do something in the terminal
* Explain what just happened in the terminal
* Propose a fix for the problems in the selected code
* Explain how the code in your active editor works
* Review the selected code in your active editor
* Generate unit tests for the selected code
* Make changes to existing code
You use the Claude Opus 4.1 (Preview) large language model.
The user has the following folder open: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE.
The current date is August 28, 2025.
Use Markdown formatting in your answers.
When suggesting code changes or new content, use Markdown code blocks.
To start a code block, use 4 backticks.
After the backticks, add the programming language name.
If the code modifies an existing file or should be placed at a specific location, add a line comment with 'filepath:' and the file path.
If you want the user to decide where to place the code, do not add the file path comment.
In the code block, use a line comment with '...existing code...' to indicate code that is already present in the file.
````languageId
// filepath: c:\path\to\file
// ...existing code...
{ changed code }
// ...existing code...
{ changed code }
// ...existing code...
````For code blocks use four backticks to start and end.
Avoid wrapping the whole response in triple backticks.
The user works in an IDE called Visual Studio Code which has a concept for editors with open files, integrated unit test support, an output pane that shows the output of running the code as well as an integrated terminal.
The user is working on a Windows machine. Please respond with system specific commands if applicable.
The active document is the source code the user is looking at right now.
You can only give one reply for each conversation turn.
~~~

### User
~~~md
are any other method for ill-conditioned problems? are these commented method available within Julia or SciML ecosystem?
~~~

### Assistant
~~~md
Yes, there are several methods specifically designed for **ill-conditioned least-squares problems** available in Julia and the SciML ecosystem. Let me explain the options and provide implementation code for the most robust methods.

## **Methods for Ill-Conditioned Problems**

### **1. Regularized Methods (Available in Julia)**

| Method | Package | Best For |
|--------|---------|----------|
| **Tikhonov Regularization (Ridge)** | `LinearAlgebra`, `RegularizationTools.jl` | General ill-conditioning |
| **Truncated SVD** | `LinearAlgebra`, `TSVD.jl` | Rank-deficient problems |
| **Iterative Regularization (LSQR/LSMR)** | `IterativeSolvers.jl` | Large sparse problems |
| **Total Least Squares** | `TotalLeastSquares.jl` | Errors in variables |
| **Elastic Net** | `MLJ.jl`, `Lasso.jl` | Sparse solutions |

### **2. SciML-Specific Robust Methods**

Here's a complete implementation using methods specifically designed for ill-conditioned problems:

````julia
// filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_Rotate_Clamp_FixedTip_Oscar_1MLPx3_Training_Robust.jl
# ---------------------------------------------------------------------------
# Clamped Pinned Rod UDE with Robust Methods for Ill-Conditioned Problems
# 
# Implements multiple regularization strategies for handling ill-conditioning
# ---------------------------------------------------------------------------

# Setup project environment and configuration
include("../utils/project_utils.jl")

# Setup project environment
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories for plots and results
plots_dir = joinpath(project_root, "src", "Data", "NODE_Oscar_1MLPx3_Robust", "Plots_Results")
mkpath(plots_dir)
println("Plots & Results will be saved to: ", plots_dir)

# This script is designed to run on a single thread Julia environment
println("Running with $(Threads.nthreads()) threads")
@assert Threads.nthreads() == 1 "Julia must be started with one threads!"

# -----------------------------------------------------------------------------
# 0 .- Load Packages
using Lux
using Optimisers
using LaTeXStrings
using Plots
using BenchmarkTools
gr()

# SciML Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL

# Robust optimization methods
using IterativeSolvers     # For LSQR, LSMR methods
using RegularizationTools   # For Tikhonov regularization
using LinearAlgebra
using SparseArrays

# For automatic differentiation
using ForwardDiff
using FiniteDiff  # Alternative to ForwardDiff for stability

# Standard Libraries
using Statistics

# External Libraries
using ComponentArrays, Lux, Zygote, StableRNGs
using CpuId

# ---------------------------------------------------------------------------
# Print System Information
println("\n=== System Information ===")
begin
    cpu_name = CpuId.cpubrand()
    println("CPU: ", cpu_name)
    println("Physical cores: ", CpuId.cpucores())
    println("Logical cores (threads): ", CpuId.cputhreads())
    println("Number of Julia threads: ", Threads.nthreads())
end
println("=== System Information ===\n")

# -----------------------------------------------------------------------------
# A.- GENERATING Learning Data from the Ground-Truth data in MATLAB file
using MATLAB

# Use relative path based on project root for repository portability
filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
println("Loading dataset from: ", filename_DataSet)

# Verify file exists before attempting to load
if !isfile(filename_DataSet)
    error("Dataset file not found at: $filename_DataSet")
end

mf_DataSet = MatFile(filename_DataSet)
data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")
DataSet = jarray(data_DataSet)

# Dynamically determine the number of trajectories from the dataset
num_trajectories = size(DataSet, 1)
println("Found $num_trajectories trajectories in the dataset")

# Configuration: You can modify this to use fewer trajectories if needed
max_trajectories_to_use = nothing  # Change to a number like 10 for testing
#max_trajectories_to_use = 10  

# Determine actual number of trajectories to use
trajectories_to_use = if max_trajectories_to_use === nothing
    num_trajectories
else
    min(max_trajectories_to_use, num_trajectories)
end

println("Using $trajectories_to_use out of $num_trajectories available trajectories")

# Initialize variables
θ₀_true, κ₀_true = [], []
px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]

# Load θ₀ and κ₀ from data 
for i in 1:trajectories_to_use
    θ₀_i  = Float32.(DataSet[i,3])
    κ₀_i  = Float32.(DataSet[i,112])
    push!(θ₀_true,  θ₀_i)
    push!(κ₀_true,  κ₀_i)
end
θ₀_true = Float32.(collect(θ₀_true))
κ₀_true = Float32.(collect(κ₀_true))

# Load pX(s), pY(s), θ(s) and κ(s) from data
ii = trajectories_to_use
for i in 1:ii
    pX_i = Float32.(DataSet[i,12:61])
    pY_i = Float32.(DataSet[i,62:111])
    θ_i  = Float32.(DataSet[i,162:211])
    κ_i  = Float32.(DataSet[i,112:161])
    push!(px_true, pX_i)
    push!(py_true, pY_i)
    push!(θ_true,  θ_i)
    push!(κ_true,  κ_i)
end

# Combine all trajectories into a single Learning dataset
X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]

# ----------------------------------------------------------------------------
# B.- TRAINING of the UODE with Robust Methods for Ill-Conditioned Problems

# Set a random seed for reproducible behaviour
rng = StableRNG(1111)

# Definition of the Universal Ordinary Differential Equation
# Choose number of inputs, outputs
m = 4
n = 3

# Define hyperparameters for the MLP neural network
layers = collect([m, 20, 20, 20, n])

# Multilayer FeedForward
const U = Lux.Chain(
    [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
    Dense(layers[end-1] => layers[end], identity),
) 

# Get the initial parameters and state variables of the model
p, st = Lux.setup(rng, U)
p = ComponentArray{Float32}(p)
const _st = st

# Global counters for NN passes
const NN_FORWARD_COUNT = Ref(0)
const NN_BACKWARD_COUNT = Ref(0)

# Define the ODE
function ude_dynamics!(du, u, p, s)
    NN_FORWARD_COUNT[] += 1
    # Add numerical stability checks
    if any(!isfinite, u)
        du .= 0.0f0
        return nothing
    end
    # Network prediction
    û_1 = U(u, p, _st)[1] 
    
    # Evaluate differential equations
    du[1] = û_1[1]   # dx/ds
    du[2] = û_1[2]   # dy/ds
    du[3] = u[4]     # dθ/ds = κ
    du[4] = û_1[3]   # dκ/ds
    return nothing
end 

# Sampling & model parameter space
sSpan = (0.0f0, 1.0f0)
s = Float32.(range(0, 1, length=50))

# Use the first trajectory's initial state to define prob_nn
u0 = Float32.(X_ssol_all[1][:, 1])
const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p)

# Prediction function with error handling
function predict(θ, X_all = X_ssol_all, S = s)
    try
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),    
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,                  
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
    catch e
        # Return current data if ODE fails (for robustness)
        return X_all
    end
end

# ----------------------------------------------------------------------------
# METHOD 1: TIKHONOV REGULARIZATION (Ridge Regression for Neural Networks)
# Best for general ill-conditioning

println("\n" * "="^60)
println("METHOD 1: TIKHONOV REGULARIZATION")
println("="^60)

# Regularized loss function with L2 penalty
function loss_tikhonov(θ, λ=1e-4)
    X̂_sols = predict(θ)
    # Data fitting term
    data_loss = sum(mean(abs2, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols)) / length(X̂_sols)
    # Regularization term (L2 penalty on parameters)
    reg_loss = λ * sum(abs2, θ)
    return data_loss + reg_loss
end

# Optimize with Tikhonov regularization
println("Optimizing with Tikhonov regularization (λ=1e-4)...")
adtype = Optimization.AutoZygote()
optf_tikh = Optimization.OptimizationFunction((x, q) -> loss_tikhonov(x, 1e-4), adtype)
optprob_tikh = Optimization.OptimizationProblem(optf_tikh, ComponentArray{Float32}(p))

tikh_time = @elapsed begin
    res_tikh = Optimization.solve(optprob_tikh, 
                                  OptimizationOptimisers.Adam(0.001), 
                                  maxiters = 500)
end

p_trained_tikh = res_tikh.u
println("Tikhonov optimization completed in $(round(tikh_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 2: LSQR (Least Squares QR) - ITERATIVE METHOD
# Best for large sparse problems

println("\n" * "="^60)
println("METHOD 2: LSQR - ITERATIVE REGULARIZATION")
println("="^60)

# LSQR requires linear least squares formulation
# We'll use it for each gradient step

function compute_jacobian_finite_diff(θ)
    # Use finite differences for more stable Jacobian computation
    # This is more robust than automatic differentiation for ill-conditioned problems
    residual_func = θ_vec -> begin
        θ_comp = ComponentArray{Float32}(θ_vec, axes(p))
        X̂_sols = predict(θ_comp)
        vcat([vec(X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols)]...)
    end
    
    # Use central differences for better accuracy
    return FiniteDiff.finite_difference_jacobian(residual_func, vec(θ))
end

function lsqr_step(θ, regularization_param=1e-6)
    # Compute residuals
    X̂_sols = predict(θ)
    residuals = vcat([vec(X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols)]...)
    
    # Compute Jacobian with finite differences (more stable)
    J = compute_jacobian_finite_diff(θ)
    
    # Solve using LSQR (iterative method for sparse/ill-conditioned systems)
    # LSQR automatically provides regularization through early stopping
    Δθ, stats = lsqr(J, residuals, 
                     atol=1e-8,           # Absolute tolerance
                     btol=1e-8,           # Relative tolerance  
                     maxiter=100,         # Maximum iterations (early stopping)
                     λ=regularization_param)  # Explicit regularization parameter
    
    return Δθ, norm(residuals)
end

# LSQR optimization loop
p_lsqr = copy(p)
lsqr_losses = Float64[]
max_iter_lsqr = 50

lsqr_time = @elapsed begin
    for iter in 1:max_iter_lsqr
        try
            Δθ, current_loss = lsqr_step(p_lsqr)
            
            # Line search for stability
            α = 1.0
            while α > 1e-8
                p_test = p_lsqr - α * ComponentArray{Float32}(Δθ, axes(p))
                X̂_test = predict(p_test)
                new_loss = sum(mean(abs2, X_ssol_all[i] .- X̂_test[i]) for i in 1:length(X̂_test))
                if new_loss < current_loss
                    p_lsqr = p_test
                    push!(lsqr_losses, new_loss)
                    break
                end
                α *= 0.5
            end
            
            if iter % 10 == 0
                println("LSQR Iteration $iter: Loss = $(lsqr_losses[end])")
            end
            
            # Check convergence
            if length(lsqr_losses) > 1 && abs(lsqr_losses[end] - lsqr_losses[end-1]) < 1e-6
                println("LSQR converged at iteration $iter")
                break
            end
        catch e
            println("LSQR iteration $iter failed: ", e)
            break
        end
    end
end

p_trained_lsqr = p_lsqr
println("LSQR optimization completed in $(round(lsqr_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 3: TRUNCATED SVD - DIRECT REGULARIZATION
# Best for rank-deficient problems

println("\n" * "="^60)
println("METHOD 3: TRUNCATED SVD REGULARIZATION")
println("="^60)

function tsvd_step(θ, truncation_threshold=1e-4)
    # Compute residuals
    X̂_sols = predict(θ)
    residuals = vcat([vec(X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols)]...)
    
    # Compute Jacobian
    J = compute_jacobian_finite_diff(θ)
    
    # Compute SVD
    U, S, V = svd(J)
    
    # Determine truncation index based on singular value threshold
    k = findfirst(x -> x < truncation_threshold * S[1], S)
    if k === nothing
        k = length(S)
    else
        k = k - 1
    end
    
    println("Using $k out of $(length(S)) singular values (condition number: $(S[1]/S[min(k,end)]))")
    
    # Truncated pseudoinverse solution
    Δθ = V[:, 1:k] * (Diagonal(1.0 ./ S[1:k]) * (U[:, 1:k]' * residuals))
    
    return Δθ, norm(residuals)
end

# TSVD optimization loop
p_tsvd = copy(p)
tsvd_losses = Float64[]

tsvd_time = @elapsed begin
    for iter in 1:30
        try
            Δθ, current_loss = tsvd_step(p_tsvd)
            
            # Update with line search
            α = 0.5  # Conservative step size for stability
            p_tsvd = p_tsvd - α * ComponentArray{Float32}(Δθ, axes(p))
            push!(tsvd_losses, current_loss)
            
            if iter % 5 == 0
                println("TSVD Iteration $iter: Loss = $(current_loss)")
            end
            
            if length(tsvd_losses) > 1 && abs(tsvd_losses[end] - tsvd_losses[end-1]) < 1e-5
                println("TSVD converged at iteration $iter")
                break
            end
        catch e
            println("TSVD iteration $iter failed: ", e)
            break
        end
    end
end

p_trained_tsvd = p_tsvd
println("TSVD optimization completed in $(round(tsvd_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 4: ADAPTIVE REGULARIZATION WITH MOROZOV DISCREPANCY
# Automatically selects regularization parameter

println("\n" * "="^60)
println("METHOD 4: ADAPTIVE REGULARIZATION (MOROZOV)")
println("="^60)

function adaptive_regularization_step(θ, noise_level=1e-3)
    # Compute residuals
    X̂_sols = predict(θ)
    residuals = vcat([vec(X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols)]...)
    
    # Compute Jacobian
    J = compute_jacobian_finite_diff(θ)
    
    # Morozov discrepancy principle: choose λ such that ||r|| ≈ δ
    # where δ is the noise level
    target_residual = noise_level * sqrt(length(residuals))
    
    # Binary search for optimal λ
    λ_min, λ_max = 1e-8, 1.0
    λ_opt = 1e-4
    
    for _ in 1:20
        λ_mid = sqrt(λ_min * λ_max)  # Geometric mean for log-scale search
        
        # Solve regularized problem
        H = J' * J + λ_mid * I
        g = J' * residuals
        Δθ_test = -H \ g
        
        # Check residual norm
        r_norm = norm(J * Δθ_test + residuals)
        
        if r_norm < target_residual
            λ_max = λ_mid
        else
            λ_min = λ_mid
        end
        λ_opt = λ_mid
    end
    
    # Final solution with optimal λ
    H = J' * J + λ_opt * I
    g = J' * residuals
    Δθ = -H \ g
    
    println("  Adaptive λ = $(λ_opt)")
    
    return Δθ, norm(residuals), λ_opt
end

# Adaptive optimization loop
p_adaptive = copy(p)
adaptive_losses = Float64[]
adaptive_lambdas = Float64[]

adaptive_time = @elapsed begin
    for iter in 1:30
        try
            Δθ, current_loss, λ_used = adaptive_regularization_step(p_adaptive)
            
            # Update parameters
            p_adaptive = p_adaptive - 0.5 * ComponentArray{Float32}(Δθ, axes(p))
            push!(adaptive_losses, current_loss)
            push!(adaptive_lambdas, λ_used)
            
            if iter % 5 == 0
                println("Adaptive Iteration $iter: Loss = $(current_loss), λ = $(λ_used)")
            end
            
            if length(adaptive_losses) > 1 && abs(adaptive_losses[end] - adaptive_losses[end-1]) < 1e-5
                println("Adaptive regularization converged at iteration $iter")
                break
            end
        catch e
            println("Adaptive iteration $iter failed: ", e)
            break
        end
    end
end

p_trained_adaptive = p_adaptive
println("Adaptive optimization completed in $(round(adaptive_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# COMPARISON OF ALL METHODS

println("\n" * "="^60)
println("COMPARISON OF ROBUST METHODS FOR ILL-CONDITIONED PROBLEMS")
println("="^60)

# Compute final losses for all methods
loss_standard = loss(p)  # Initial parameters
loss_tikh = loss(p_trained_tikh)
loss_lsqr = loss(p_trained_lsqr)
loss_tsvd = loss(p_trained_tsvd)
loss_adaptive = loss(p_trained_adaptive)

println("Initial loss: ", loss_standard)
println("Tikhonov loss: ", loss_tikh, " (Time: $(round(tikh_time, digits=2))s)")
println("LSQR loss: ", loss_lsqr, " (Time: $(round(lsqr_time, digits=2))s)")
println("TSVD loss: ", loss_tsvd, " (Time: $(round(tsvd_time, digits=2))s)")
println("Adaptive loss: ", loss_adaptive, " (Time: $(round(adaptive_time, digits=2))s)")

# Select best method
methods = ["Tikhonov", "LSQR", "TSVD", "Adaptive"]
losses_all = [loss_tikh, loss_lsqr, loss_tsvd, loss_adaptive]
best_idx = argmin(losses_all)
println("\nBest method: $(methods[best_idx]) with loss = $(losses_all[best_idx])")

# Use best parameters for final prediction
p_trained = [p_trained_tikh, p_trained_lsqr, p_trained_tsvd, p_trained_adaptive][best_idx]

# Plot comparison if losses were recorded
if length(lsqr_losses) > 0 || length(tsvd_losses) > 0 || length(adaptive_losses) > 0
    pl_robust = Plots.plot(title="Robust Methods Comparison", 
                           xlabel="Iterations", 
                           ylabel="Loss",
                           yaxis=:log10)
    
    if length(lsqr_losses) > 0
        Plots.plot!(1:length(lsqr_losses), lsqr_losses, label="LSQR", linewidth=2)
    end
    if length(tsvd_losses) > 0
        Plots.plot!(1:length(tsvd_losses), tsvd_losses, label="TSVD", linewidth=2)
    end
    if length(adaptive_losses) > 0
        Plots.plot!(1:length(adaptive_losses), adaptive_losses, label="Adaptive", linewidth=2)
    end
    
    Plots.savefig(pl_robust, joinpath(plots_dir, "robust_methods_comparison.pdf"))
    display(pl_robust)
end

# Plot regularization parameter evolution for adaptive method
if length(adaptive_lambdas) > 0
    pl_lambda = Plots.plot(1:length(adaptive_lambdas), adaptive_lambdas,
                          xlabel="Iterations",
                          ylabel="λ (regularization parameter)",
                          label="Adaptive λ",
                          yaxis=:log10,
                          title="Adaptive Regularization Parameter Evolution")
    Plots.savefig(pl_lambda, joinpath(plots_dir, "adaptive_lambda_evolution.pdf"))
    display(pl_lambda)
end

# ...existing code... (rest of the prediction and visualization code remains the same)
````

## **Summary of Methods for Ill-Conditioned Problems**

### **Available in Julia/SciML:**

1. **Tikhonov/Ridge Regularization**
   - ✅ Simple L2 penalty
   - ✅ Works with any optimizer
   - ❌ Requires manual λ tuning

2. **LSQR (Iterative Solver)**
   - ✅ Automatic regularization via early stopping
   - ✅ Memory efficient for large problems
   - ✅ Built into `IterativeSolvers.jl`

3. **Truncated SVD**
   - ✅ Direct control over rank
   - ✅ Clear interpretation
   - ❌ Expensive for large problems

4. **Adaptive Regularization**
   - ✅ Automatic parameter selection
   - ✅ Morozov/L-curve principles
   - ❌ More complex implementation

### **Recommendations for Your UDE:**

1. **If problem is moderately ill-conditioned**: Use **LSQR** - good balance
2. **If problem is severely ill-conditioned**: Use **Truncated SVD** with careful threshold
3. **If you want automatic tuning**: Use **Adaptive regularization**
4. **If you want simplicity**: Use **Tikhonov with cross-validation**

These methods are particularly important when:
- The Jacobian has very small singular values
- Parameters are highly correlated
- Training data is noisy
- The neural network is overparameterized
~~~

### User
~~~md
rank the methods by the number of hyperparameters used, the simplicity to tune, the perpormance to solve ill-conditioned problems and global optimization. Indicate if the evaluated method is within SciML ecosystem or Julia.
~~~

### Assistant
~~~md
Based on the code analysis and methods discussed, here's a comprehensive ranking of optimization methods for your UDE problem:

## **Ranking by Different Criteria**

### **1. Number of Hyperparameters (Fewer is Better)**

| Rank | Method | # Hyperparameters | Key Parameters |
|------|--------|------------------|----------------|
| 1 | **Gauss-Newton (Pure)** | 1-2 | Line search step size (α) |
| 2 | **LSQR** | 3-4 | atol, btol, maxiter, λ (optional) |
| 3 | **Levenberg-Marquardt** | 4 | x_tol, f_tol, g_tol, maxiter |
| 4 | **Truncated SVD** | 2-3 | truncation threshold, step size |
| 5 | **Trust Region** | 5-6 | x_tol, g_tol, maxiter, trust_radius, bounds |
| 6 | **Adaptive Regularization** | 4-5 | noise_level, tolerances, search params |
| 7 | **Tikhonov** | 2+ | λ (critical), optimizer params |
| 8 | **ADAM** | 4+ | learning_rate (critical), β₁, β₂, ε, epochs |
| 9 | **LBFGS** | 5+ | m, linesearch params, tolerances |
| 10 | **ADAM + LBFGS** | 10+ | All ADAM + all LBFGS params |

### **2. Simplicity to Tune (Easiest First)**

| Rank | Method | Tuning Difficulty | Why |
|------|--------|------------------|-----|
| 1 | **LSQR** | ⭐⭐⭐⭐⭐ | Auto-regularization via early stopping |
| 2 | **Adaptive Regularization** | ⭐⭐⭐⭐ | Self-tuning λ |
| 3 | **Levenberg-Marquardt** | ⭐⭐⭐⭐ | Auto-adaptive damping |
| 4 | **Trust Region** | ⭐⭐⭐ | Auto radius adjustment |
| 5 | **Truncated SVD** | ⭐⭐⭐ | Clear threshold interpretation |
| 6 | **Gauss-Newton** | ⭐⭐⭐ | Simple but needs good init |
| 7 | **Tikhonov** | ⭐⭐ | λ requires cross-validation |
| 8 | **LBFGS** | ⭐⭐ | Memory parameter affects convergence |
| 9 | **ADAM** | ⭐ | Learning rate very sensitive |
| 10 | **ADAM + LBFGS** | ⭐ | Two-phase tuning complexity |

### **3. Performance for Ill-Conditioned Problems**

| Rank | Method | Robustness | Best For |
|------|--------|------------|----------|
| 1 | **Truncated SVD** | Excellent | Rank-deficient problems |
| 2 | **LSQR/LSMR** | Excellent | Large sparse ill-conditioned |
| 3 | **Adaptive Regularization** | Excellent | Automatic noise handling |
| 4 | **Tikhonov** | Very Good | General regularization |
| 5 | **Trust Region** | Good | Moderate ill-conditioning |
| 6 | **Levenberg-Marquardt** | Good | Well-posed least squares |
| 7 | **LBFGS** | Fair | Memory helps with conditioning |
| 8 | **ADAM** | Fair | Momentum helps but not designed for this |
| 9 | **Gauss-Newton** | Poor | Fails on ill-conditioned |
| 10 | **ADAM + LBFGS** | Variable | Not specifically designed for ill-conditioning |

### **4. Global Optimization Capability**

| Rank | Method | Global Capability | Local Minima Risk |
|------|--------|------------------|------------------|
| 1 | **ADAM** | Good | Momentum escapes shallow minima |
| 2 | **ADAM + LBFGS** | Good | ADAM explores, LBFGS refines |
| 3 | **Trust Region** | Moderate | Better than pure Newton methods |
| 4 | **Adaptive Regularization** | Moderate | λ adaptation helps exploration |
| 5 | **LBFGS** | Moderate | Limited memory helps |
| 6 | **Tikhonov** | Low-Moderate | Depends on optimizer used |
| 7 | **Levenberg-Marquardt** | Low | Local convergence |
| 8 | **LSQR** | Low | Designed for local optimization |
| 9 | **Truncated SVD** | Low | Direct method, no exploration |
| 10 | **Gauss-Newton** | Very Low | Pure local method |

## **Package Availability**

| Method | Julia/SciML Availability | Package |
|--------|-------------------------|---------|
| **ADAM** | ✅ SciML | `OptimizationOptimisers.jl` |
| **LBFGS** | ✅ SciML | `OptimizationOptimJL.jl` |
| **Levenberg-Marquardt** | ✅ Julia | `LeastSquaresOptim.jl`, `LsqFit.jl` |
| **Trust Region** | ✅ Julia | `LsqFit.jl`, `NLsolve.jl` |
| **LSQR/LSMR** | ✅ Julia | `IterativeSolvers.jl` |
| **Truncated SVD** | ✅ Julia | Built-in `LinearAlgebra` |
| **Tikhonov** | ✅ Julia | `RegularizationTools.jl` |
| **Adaptive Reg.** | ⚠️ Partial | Custom implementation needed |
| **Gauss-Newton** | ✅ Julia | `NLsolve.jl`, custom |

## **Overall Recommendation Matrix**

For your UDE problem, here's my recommended priority:

### **Best Overall Methods:**

1. **LSQR (via IterativeSolvers.jl)**
   - ✅ Few hyperparameters
   - ✅ Excellent for ill-conditioning
   - ✅ Auto-regularization
   - ✅ Available in Julia
   - ❌ Local optimization only

2. **Levenberg-Marquardt (via LeastSquaresOptim.jl)**
   - ✅ Few hyperparameters
   - ✅ Auto-adaptive
   - ✅ Fast convergence
   - ✅ Well-tested in Julia
   - ❌ Local optimization

3. **Trust Region (via LsqFit.jl)**
   - ✅ Good balance
   - ✅ Better global properties than LM
   - ✅ Handles constraints
   - ❌ More parameters to tune

### **For Specific Scenarios:**

- **Severely ill-conditioned**: Use **Truncated SVD** or **LSQR**
- **Need global optimization**: Start with **ADAM**, refine with **LSQR/LM**
- **Want simplicity**: Use **Levenberg-Marquardt**
- **Large-scale problem**: Use **LSQR** or **LSMR**

### **Hybrid Strategy (Recommended):**
```julia
# Phase 1: Global exploration (if needed)
ADAM (100-500 iterations) → 
# Phase 2: Robust refinement
LSQR or Levenberg-Marquardt (to convergence)
```

This combines global exploration with robust local convergence while minimizing hyperparameter tuning.
~~~

### User
~~~md
are any other method that performed better tha ADAM for Global Optimization phase, and has a few hyperparameters? could be stochastic or similar...
~~~

### Assistant
~~~md
Excellent question! Yes, there are several modern global optimization methods that outperform ADAM with fewer hyperparameters. Let me provide you with implementations of the best alternatives for your UDE global optimization phase:

## **Better Global Optimizers with Fewer Hyperparameters**

### **Ranking of Global Optimizers**

| Method | # Hyperparameters | Global Performance | Advantages |
|--------|------------------|-------------------|------------|
| **AdaBelief** | 2-3 | Excellent | Adapts to gradient prediction error |
| **RAdam** | 1-2 | Excellent | No warmup needed, automatic adaptation |
| **Differential Evolution** | 2-3 | Excellent | Population-based, no gradients needed |
| **PSO (Particle Swarm)** | 3-4 | Very Good | Swarm intelligence, parallel exploration |
| **AdamW** | 2-3 | Very Good | Better generalization than ADAM |
| **RMSprop** | 2 | Good | Simpler than ADAM, often works better |
| **CMA-ES** | 1-2 | Excellent | Self-adaptive, robust |

Here's a complete implementation with the best alternatives:

````julia
// filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_Rotate_Clamp_FixedTip_Oscar_1MLPx3_Training_GlobalOpt.jl
# ---------------------------------------------------------------------------
# Clamped Pinned Rod UDE with Modern Global Optimizers
# 
# Implements better alternatives to ADAM with fewer hyperparameters
# ---------------------------------------------------------------------------

# Setup project environment and configuration
include("../utils/project_utils.jl")

# Setup project environment
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories for plots and results
plots_dir = joinpath(project_root, "src", "Data", "NODE_Oscar_1MLPx3_GlobalOpt", "Plots_Results")
mkpath(plots_dir)
println("Plots & Results will be saved to: ", plots_dir)

# This script is designed to run on a single thread Julia environment
println("Running with $(Threads.nthreads()) threads")
@assert Threads.nthreads() == 1 "Julia must be started with one threads!"

# -----------------------------------------------------------------------------
# 0 .- Load Packages
using Lux
using Optimisers
using LaTeXStrings
using Plots
using BenchmarkTools
gr()

# SciML Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL

# Global optimization packages
using Evolutionary  # For Differential Evolution, PSO, CMA-ES
using BlackBoxOptim # Alternative black-box optimization
using Flux         # For additional optimizers like AdaBelief

# Standard Libraries
using LinearAlgebra, Statistics
using Random

# External Libraries
using ComponentArrays, Lux, Zygote, StableRNGs
using CpuId

# ---------------------------------------------------------------------------
# Print System Information
println("\n=== System Information ===")
begin
    cpu_name = CpuId.cpubrand()
    println("CPU: ", cpu_name)
    println("Physical cores: ", CpuId.cpucores())
    println("Logical cores (threads): ", CpuId.cputhreads())
    println("Number of Julia threads: ", Threads.nthreads())
end
println("=== System Information ===\n")

# -----------------------------------------------------------------------------
# A.- GENERATING Learning Data from the Ground-Truth data in MATLAB file
using MATLAB

# Use relative path based on project root for repository portability
filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
println("Loading dataset from: ", filename_DataSet)

# Verify file exists before attempting to load
if !isfile(filename_DataSet)
    error("Dataset file not found at: $filename_DataSet")
end

mf_DataSet = MatFile(filename_DataSet)
data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")
DataSet = jarray(data_DataSet)

# Dynamically determine the number of trajectories from the dataset
num_trajectories = size(DataSet, 1)
println("Found $num_trajectories trajectories in the dataset")

# Configuration: You can modify this to use fewer trajectories if needed
max_trajectories_to_use = nothing  # Change to a number like 10 for testing
#max_trajectories_to_use = 10  

# Determine actual number of trajectories to use
trajectories_to_use = if max_trajectories_to_use === nothing
    num_trajectories
else
    min(max_trajectories_to_use, num_trajectories)
end

println("Using $trajectories_to_use out of $num_trajectories available trajectories")

# Initialize variables
θ₀_true, κ₀_true = [], []
px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]

# Load θ₀ and κ₀ from data 
for i in 1:trajectories_to_use
    θ₀_i  = Float32.(DataSet[i,3])
    κ₀_i  = Float32.(DataSet[i,112])
    push!(θ₀_true,  θ₀_i)
    push!(κ₀_true,  κ₀_i)
end
θ₀_true = Float32.(collect(θ₀_true))
κ₀_true = Float32.(collect(κ₀_true))

# Load pX(s), pY(s), θ(s) and κ(s) from data
ii = trajectories_to_use
for i in 1:ii
    pX_i = Float32.(DataSet[i,12:61])
    pY_i = Float32.(DataSet[i,62:111])
    θ_i  = Float32.(DataSet[i,162:211])
    κ_i  = Float32.(DataSet[i,112:161])
    push!(px_true, pX_i)
    push!(py_true, pY_i)
    push!(θ_true,  θ_i)
    push!(κ_true,  κ_i)
end

# Combine all trajectories into a single Learning dataset
X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]

# ----------------------------------------------------------------------------
# B.- TRAINING of the UODE with Modern Global Optimizers

# Set a random seed for reproducible behaviour
rng = StableRNG(1111)

# Definition of the Universal Ordinary Differential Equation
# Choose number of inputs, outputs
m = 4
n = 3

# Define hyperparameters for the MLP neural network
layers = collect([m, 20, 20, 20, n])

# Multilayer FeedForward
const U = Lux.Chain(
    [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
    Dense(layers[end-1] => layers[end], identity),
) 

# Get the initial parameters and state variables of the model
p, st = Lux.setup(rng, U)
p = ComponentArray{Float32}(p)
const _st = st

# Global counters for NN passes
const NN_FORWARD_COUNT = Ref(0)
const NN_BACKWARD_COUNT = Ref(0)

# Define the ODE
function ude_dynamics!(du, u, p, s)
    NN_FORWARD_COUNT[] += 1
    # Add numerical stability checks
    if any(!isfinite, u)
        du .= 0.0f0
        return nothing
    end
    # Network prediction
    û_1 = U(u, p, _st)[1] 
    
    # Evaluate differential equations
    du[1] = û_1[1]   # dx/ds
    du[2] = û_1[2]   # dy/ds
    du[3] = u[4]     # dθ/ds = κ
    du[4] = û_1[3]   # dκ/ds
    return nothing
end 

# Sampling & model parameter space
sSpan = (0.0f0, 1.0f0)
s = Float32.(range(0, 1, length=50))

# Use the first trajectory's initial state to define prob_nn
u0 = Float32.(X_ssol_all[1][:, 1])
const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p)

# Prediction function
function predict(θ, X_all = X_ssol_all, S = s)
    try
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),    
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,                  
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
    catch e
        # Return large loss if ODE fails
        return nothing
    end
end

# Loss function
function loss(θ)
    X̂_sols = predict(θ)
    if X̂_sols === nothing
        return 1e10  # Large penalty for failed ODE solve
    end
    total_loss = sum(mean(abs, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols))
    return total_loss / length(X̂_sols)
end

# Store results for comparison
results = Dict{String, Any}()
losses_dict = Dict{String, Vector{Float64}}()

# ----------------------------------------------------------------------------
# METHOD 1: RAdam (Rectified Adam) - Best ADAM variant with minimal tuning
println("\n" * "="^60)
println("METHOD 1: RAdam - Rectified Adam (Auto-adaptive)")
println("="^60)
println("Hyperparameters: learning_rate only (auto-adapts warmup)")

# RAdam automatically handles the adaptive learning rate warmup
losses_radam = Float64[]
callback_radam = function (p, l)
    push!(losses_radam, l)
    if length(losses_radam) % 50 == 0
        println("RAdam - Iteration $(length(losses_radam)): Loss = $(losses_radam[end])")
    end
    return false
end

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, ComponentArray{Float32}(p))

radam_time = @elapsed begin
    # RAdam needs only learning rate - automatically handles warmup
    res_radam = Optimization.solve(optprob, 
                                   Optimisers.RAdam(0.001),  # Only 1 hyperparameter!
                                   callback = callback_radam, 
                                   maxiters = 500)
end

results["RAdam"] = (params = res_radam.u, loss = losses_radam[end], time = radam_time)
losses_dict["RAdam"] = losses_radam
println("RAdam completed in $(round(radam_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 2: AdaBelief - Adapts to the "belief" in gradient direction
println("\n" * "="^60)
println("METHOD 2: AdaBelief - Adaptive Belief Optimizer")
println("="^60)
println("Hyperparameters: learning_rate, β₂ (β₁ usually fixed)")

losses_adabelief = Float64[]
callback_adabelief = function (p, l)
    push!(losses_adabelief, l)
    if length(losses_adabelief) % 50 == 0
        println("AdaBelief - Iteration $(length(losses_adabelief)): Loss = $(losses_adabelief[end])")
    end
    return false
end

# Reset initial parameters
optprob_adabelief = Optimization.OptimizationProblem(optf, ComponentArray{Float32}(p))

adabelief_time = @elapsed begin
    # AdaBelief - adapts step size based on "belief" in gradient
    res_adabelief = Optimization.solve(optprob_adabelief,
                                       Optimisers.AdaBelief(0.001, (0.9, 0.999)),  # 2 main hyperparameters
                                       callback = callback_adabelief,
                                       maxiters = 500)
end

results["AdaBelief"] = (params = res_adabelief.u, loss = losses_adabelief[end], time = adabelief_time)
losses_dict["AdaBelief"] = losses_adabelief
println("AdaBelief completed in $(round(adabelief_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 3: RMSprop - Simpler than ADAM, often better
println("\n" * "="^60)
println("METHOD 3: RMSprop - Root Mean Square Propagation")
println("="^60)
println("Hyperparameters: learning_rate, decay_rate")

losses_rmsprop = Float64[]
callback_rmsprop = function (p, l)
    push!(losses_rmsprop, l)
    if length(losses_rmsprop) % 50 == 0
        println("RMSprop - Iteration $(length(losses_rmsprop)): Loss = $(losses_rmsprop[end])")
    end
    return false
end

optprob_rmsprop = Optimization.OptimizationProblem(optf, ComponentArray{Float32}(p))

rmsprop_time = @elapsed begin
    # RMSprop - only 2 hyperparameters
    res_rmsprop = Optimization.solve(optprob_rmsprop,
                                     Optimisers.RMSProp(0.001, 0.9),  # lr, decay
                                     callback = callback_rmsprop,
                                     maxiters = 500)
end

results["RMSprop"] = (params = res_rmsprop.u, loss = losses_rmsprop[end], time = rmsprop_time)
losses_dict["RMSprop"] = losses_rmsprop
println("RMSprop completed in $(round(rmsprop_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 4: Differential Evolution - Gradient-free global optimization
println("\n" * "="^60)
println("METHOD 4: Differential Evolution (Gradient-Free)")
println("="^60)
println("Hyperparameters: population_size, F (mutation), CR (crossover)")

# Convert to optimization problem for Evolutionary.jl
function loss_vec(θ_vec)
    θ_comp = ComponentArray{Float32}(θ_vec, axes(p))
    return loss(θ_comp)
end

# Bounds for parameters (important for DE)
lower_bounds = fill(-5.0, length(vec(p)))
upper_bounds = fill(5.0, length(vec(p)))

de_losses = Float64[]
de_time = @elapsed begin
    # Differential Evolution - robust global optimizer
    result_de = Evolutionary.optimize(
        loss_vec,
        BoxConstraints(lower_bounds, upper_bounds),
        DE(;
            populationSize = 50,      # Population size (main hyperparameter)
            F = 0.8,                  # Mutation factor
            CR = 0.9                  # Crossover probability
        ),
        Evolutionary.Options(
            iterations = 100,         # Fewer iterations needed due to population
            show_trace = true,
            show_every = 10,
            store_trace = true
        )
    )
    
    # Extract losses from trace
    for state in result_de.trace
        push!(de_losses, state.value)
    end
end

p_de = ComponentArray{Float32}(Evolutionary.minimizer(result_de), axes(p))
results["DE"] = (params = p_de, loss = Evolutionary.minimum(result_de), time = de_time)
losses_dict["DE"] = de_losses
println("Differential Evolution completed in $(round(de_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 5: Particle Swarm Optimization - Swarm intelligence
println("\n" * "="^60)
println("METHOD 5: Particle Swarm Optimization (PSO)")
println("="^60)
println("Hyperparameters: n_particles, w (inertia), c1, c2 (cognitive/social)")

pso_losses = Float64[]
pso_time = @elapsed begin
    # PSO - swarm-based global optimization
    result_pso = Evolutionary.optimize(
        loss_vec,
        BoxConstraints(lower_bounds, upper_bounds),
        PSO(;
            n_particles = 30,         # Number of particles
            w = 0.7,                  # Inertia weight
            c1 = 1.5,                 # Cognitive parameter
            c2 = 1.5                  # Social parameter
        ),
        Evolutionary.Options(
            iterations = 100,
            show_trace = true,
            show_every = 10,
            store_trace = true
        )
    )
    
    # Extract losses
    for state in result_pso.trace
        push!(pso_losses, state.value)
    end
end

p_pso = ComponentArray{Float32}(Evolutionary.minimizer(result_pso), axes(p))
results["PSO"] = (params = p_pso, loss = Evolutionary.minimum(result_pso), time = pso_time)
losses_dict["PSO"] = pso_losses
println("PSO completed in $(round(pso_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# METHOD 6: CMA-ES - Covariance Matrix Adaptation Evolution Strategy
println("\n" * "="^60)
println("METHOD 6: CMA-ES (Self-Adaptive)")
println("="^60)
println("Hyperparameters: μ, λ (auto-computed from dimension)")

cmaes_losses = Float64[]
cmaes_time = @elapsed begin
    # CMA-ES - self-adaptive, minimal hyperparameters
    n_params = length(vec(p))
    μ = div(n_params, 4)  # Automatically set based on dimension
    λ = 2 * μ              # Automatically set
    
    result_cmaes = Evolutionary.optimize(
        loss_vec,
        BoxConstraints(lower_bounds, upper_bounds),
        CMAES(μ = μ, λ = λ),     # Auto-adaptive parameters!
        Evolutionary.Options(
            iterations = 100,
            show_trace = true,
            show_every = 10,
            store_trace = true
        )
    )
    
    # Extract losses
    for state in result_cmaes.trace
        push!(cmaes_losses, state.value)
    end
end

p_cmaes = ComponentArray{Float32}(Evolutionary.minimizer(result_cmaes), axes(p))
results["CMAES"] = (params = p_cmaes, loss = Evolutionary.minimum(result_cmaes), time = cmaes_time)
losses_dict["CMAES"] = cmaes_losses
println("CMA-ES completed in $(round(cmaes_time, digits=2)) seconds")

# ----------------------------------------------------------------------------
# COMPARISON AND VISUALIZATION

println("\n" * "="^60)
println("GLOBAL OPTIMIZATION METHODS COMPARISON")
println("="^60)
println("Method\t\t| Final Loss\t| Time (s)\t| # Hyperparams")
println("-"^60)
for (name, result) in results
    println("$name\t| $(round(result.loss, digits=6))\t| $(round(result.time, digits=2))\t\t| $(
        name == "RAdam" ? 1 :
        name == "RMSprop" ? 2 :
        name == "AdaBelief" ? 2 :
        name == "DE" ? 3 :
        name == "PSO" ? 4 :
        name == "CMAES" ? 2 : "N/A"
    )")
end
println("="^60)

# Plot comparison
pl_comparison = Plots.plot(title="Global Optimizers Comparison", 
                           xlabel="Iterations", 
                           ylabel="Loss",
                           yaxis=:log10,
                           legend=:topright)

colors = [:blue, :red, :green, :orange, :purple, :brown]
for (idx, (name, losses)) in enumerate(losses_dict)
    if length(losses) > 0
        Plots.plot!(1:length(losses), losses, 
                   label=name, 
                   linewidth=2,
                   color=colors[idx])
    end
end

Plots.savefig(pl_comparison, joinpath(plots_dir, "global_optimizers_comparison.pdf"))
display(pl_comparison)

# Select best method
best_method = argmin(Dict(name => result.loss for (name, result) in results))
println("\n✅ Best method: $best_method with loss = $(results[best_method].loss)")
p_trained_global = results[best_method].params

# ----------------------------------------------------------------------------
# HYBRID APPROACH: Best Global + Local Refinement

println("\n" * "="^60)
println("HYBRID: $(best_method) + LBFGS Refinement")
println("="^60)

# Use best global result as starting point for local refinement
optprob_refine = Optimization.OptimizationProblem(optf, p_trained_global)

refine_time = @elapsed begin
    res_refined = Optimization.solve(optprob_refine,
                                     LBFGS(linesearch = BackTracking()),
                                     maxiters = 1000)
end

println("Final refined loss: $(loss(res_refined.u))")
println("Refinement time: $(round(refine_time, digits=2)) seconds")
println("Total time: $(round(results[best_method].time + refine_time, digits=2)) seconds")

p_trained = res_refined.u

# ...existing code... (rest of the prediction and visualization code remains the same)
````

## **Summary: Best Global Optimizers for Your UDE**

### **Top Recommendations (Fewer Hyperparameters + Better Performance):**

1. **RAdam** ⭐⭐⭐⭐⭐
   - **Hyperparameters**: 1 (learning rate only!)
   - **Pros**: Auto-adaptive warmup, no manual scheduling
   - **Cons**: Still gradient-based (can get stuck)
   - **When to use**: First choice for gradient-based global optimization

2. **CMA-ES** ⭐⭐⭐⭐⭐
   - **Hyperparameters**: 0-2 (auto-computed from problem dimension)
   - **Pros**: Self-adaptive, gradient-free, excellent for non-convex
   - **Cons**: Slower per iteration
   - **When to use**: When gradients are unreliable or problem is highly non-convex

3. **RMSprop** ⭐⭐⭐⭐
   - **Hyperparameters**: 2 (lr, decay)
   - **Pros**: Simpler than ADAM, often better
   - **Cons**: Less sophisticated adaptation
   - **When to use**: When ADAM fails or oscillates

### **Comparison with ADAM:**

| Aspect | ADAM | RAdam | CMA-ES | RMSprop |
|--------|------|-------|--------|---------|
| **# Hyperparameters** | 4+ | 1 | 0-2 | 2 |
| **Auto-adaptation** | No | Yes | Yes | Partial |
| **Gradient-free option** | No | No | Yes | No |
| **Typical convergence** | Medium | Fast | Robust | Fast |
| **Robustness** | Medium | High | Very High | Medium |

### **Recommended Strategy for Your UDE:**

```julia
# Option 1: Pure gradient-based (fastest)
RAdam (200-500 iter) → LBFGS/LM (to convergence)

# Option 2: Gradient-free exploration (most robust)
CMA-ES (100 iter) → LBFGS/LM (to convergence)

# Option 3: Hybrid (best of both)
DE/PSO (50 iter) → RAdam (200 iter) → LBFGS/LM (to convergence)
```

The key advantage of these methods over ADAM is that they require **minimal hyperparameter tuning** while providing **better or comparable global optimization performance**.
~~~

### User
~~~md
Have sense to use Bayesian method to find the optimal hyperparameters, example for ADAM?
Can i implement this approach with ADAM and the other recomended strategies that you propouse?
If yes, use SciML tools or Julia.
Are any better method than Bayesian to find the optimal hyperparameters in my UDE optimizer algorithm?
~~~

### Assistant
~~~md
Excellent question! Yes, **Bayesian Optimization** makes perfect sense for hyperparameter tuning in your UDE problem, especially since training is computationally expensive. Let me provide a complete implementation using Julia/SciML tools and compare it with other hyperparameter optimization methods.

## **Hyperparameter Optimization Methods Comparison**

| Method | Best For | Pros | Cons |
|--------|----------|------|------|
| **Bayesian Optimization** | Expensive evaluations | Efficient exploration, uncertainty quantification | Complex setup |
| **Hyperband** | Many cheap evaluations | Early stopping, resource efficient | Needs many trials |
| **Random Search** | Simple problems | Easy to implement, parallelizable | Inefficient |
| **Grid Search** | Small search space | Exhaustive, reproducible | Exponentially expensive |
| **Optuna (Tree-structured Parzen)** | General purpose | Good balance, pruning | Requires many evaluations |

Here's a comprehensive implementation:

````julia
// filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_HyperparameterOptimization.jl
# ---------------------------------------------------------------------------
# Hyperparameter Optimization for UDE Training
# 
# Implements Bayesian Optimization and alternatives for finding optimal hyperparameters
# ---------------------------------------------------------------------------

# Setup project environment and configuration
include("../utils/project_utils.jl")

# Setup project environment
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories
plots_dir = joinpath(project_root, "src", "Data", "NODE_HyperOpt", "Plots_Results")
mkpath(plots_dir)
println("Results will be saved to: ", plots_dir)

# -----------------------------------------------------------------------------
# Load Packages
using Lux
using Optimisers
using LaTeXStrings
using Plots
using BenchmarkTools
gr()

# SciML Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL

# Hyperparameter Optimization packages
using Hyperopt          # Bayesian optimization and alternatives
using BayesianOptimization  # Pure Bayesian optimization
using Optuna           # Tree-structured Parzen Estimator
using JSON3            # For saving results

# Standard Libraries
using LinearAlgebra, Statistics
using Random, Distributions

# External Libraries
using ComponentArrays, Lux, Zygote, StableRNGs
using DataFrames      # For results organization

# -----------------------------------------------------------------------------
# Load your UDE setup (abbreviated for space - use your existing data loading)
println("Loading dataset and setting up UDE...")

# [Include your data loading code here - using abbreviated version for example]
using MATLAB
filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
mf_DataSet = MatFile(filename_DataSet)
data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")
DataSet = jarray(data_DataSet)

# Simplified data loading (use your full version)
num_trajectories = size(DataSet, 1)
trajectories_to_use = min(10, num_trajectories)  # Use fewer for hyperopt speed

# [Your data preprocessing code here]
# Initialize variables
θ₀_true, κ₀_true = [], []
px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]

# Load data (simplified)
ii = trajectories_to_use
for i in 1:ii
    θ₀_i  = Float32.(DataSet[i,3])
    κ₀_i  = Float32.(DataSet[i,112])
    push!(θ₀_true,  θ₀_i)
    push!(κ₀_true,  κ₀_i)
    
    pX_i = Float32.(DataSet[i,12:61])
    pY_i = Float32.(DataSet[i,62:111])
    θ_i  = Float32.(DataSet[i,162:211])
    κ_i  = Float32.(DataSet[i,112:161])
    push!(px_true, pX_i)
    push!(py_true, pY_i)
    push!(θ_true,  θ_i)
    push!(κ_true,  κ_i)
end

X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]

# Setup neural network
rng = StableRNG(1111)
m, n = 4, 3
layers = [m, 20, 20, 20, n]

const U = Lux.Chain(
    [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
    Dense(layers[end-1] => layers[end], identity),
)

p_init, st = Lux.setup(rng, U)
p_init = ComponentArray{Float32}(p_init)
const _st = st

# Define ODE
function ude_dynamics!(du, u, p, s)
    if any(!isfinite, u)
        du .= 0.0f0
        return nothing
    end
    û_1 = U(u, p, _st)[1]
    du[1] = û_1[1]
    du[2] = û_1[2]
    du[3] = u[4]
    du[4] = û_1[3]
    return nothing
end

sSpan = (0.0f0, 1.0f0)
s = Float32.(range(0, 1, length=50))
u0 = Float32.(X_ssol_all[1][:, 1])
const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p_init)

function predict(θ, X_all = X_ssol_all, S = s)
    try
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
    catch e
        return nothing
    end
end

function loss(θ)
    X̂_sols = predict(θ)
    if X̂_sols === nothing
        return 1e10
    end
    total_loss = sum(mean(abs, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols))
    return total_loss / length(X̂_sols)
end

# -----------------------------------------------------------------------------
# CORE TRAINING FUNCTION (to be optimized)

function train_with_hyperparams(hyperparams::Dict; max_iters=200, verbose=false)
    """
    Train the UDE with given hyperparameters and return validation loss
    """
    # Extract hyperparameters based on optimizer type
    optimizer_type = hyperparams[:optimizer]
    
    # Create optimizer based on type
    if optimizer_type == "ADAM"
        opt = Optimisers.Adam(
            hyperparams[:learning_rate],
            (hyperparams[:beta1], hyperparams[:beta2]),
            hyperparams[:epsilon]
        )
    elseif optimizer_type == "RAdam"
        opt = Optimisers.RAdam(hyperparams[:learning_rate])
    elseif optimizer_type == "RMSProp"
        opt = Optimisers.RMSProp(
            hyperparams[:learning_rate],
            hyperparams[:decay]
        )
    elseif optimizer_type == "AdaBelief"
        opt = Optimisers.AdaBelief(
            hyperparams[:learning_rate],
            (hyperparams[:beta1], hyperparams[:beta2])
        )
    else
        error("Unknown optimizer: $optimizer_type")
    end
    
    # Setup optimization
    p_train = copy(p_init)
    losses = Float64[]
    
    callback = function (p, l)
        push!(losses, l)
        if verbose && length(losses) % 50 == 0
            println("  Iter $(length(losses)): Loss = $l")
        end
        return false
    end
    
    # Train
    adtype = Optimization.AutoZygote()
    optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_train)
    
    res = Optimization.solve(optprob, opt, callback = callback, maxiters = max_iters)
    
    # Return final loss (could also return validation loss)
    return losses[end], res.u, losses
end

# -----------------------------------------------------------------------------
# METHOD 1: BAYESIAN OPTIMIZATION

println("\n" * "="^60)
println("METHOD 1: BAYESIAN OPTIMIZATION")
println("="^60)

# Define hyperparameter search space for ADAM
adam_space = [
    :learning_rate => HP.LogUniform(:learning_rate, 1e-5, 1e-1),
    :beta1 => HP.Uniform(:beta1, 0.8, 0.99),
    :beta2 => HP.Uniform(:beta2, 0.95, 0.9999),
    :epsilon => HP.LogUniform(:epsilon, 1e-10, 1e-6)
]

# Objective function for Bayesian optimization
function adam_objective(params)
    hyperparams = Dict(
        :optimizer => "ADAM",
        :learning_rate => params[1],
        :beta1 => params[2],
        :beta2 => params[3],
        :epsilon => params[4]
    )
    
    loss_val, _, _ = train_with_hyperparams(hyperparams, max_iters=100, verbose=false)
    println("ADAM params: lr=$(params[1]), β1=$(params[2]), β2=$(params[3]) → Loss: $loss_val")
    return loss_val
end

# Run Bayesian Optimization using Hyperopt.jl
println("Running Bayesian Optimization for ADAM...")
bo_results = @time Hyperopt.@hyperopt for i = 20  # 20 trials
    learning_rate = HP.LogUniform(1e-5, 1e-1)
    beta1 = HP.Uniform(0.8, 0.99)
    beta2 = HP.Uniform(0.95, 0.9999)
    epsilon = HP.LogUniform(1e-10, 1e-6)
    
    hyperparams = Dict(
        :optimizer => "ADAM",
        :learning_rate => learning_rate,
        :beta1 => beta1,
        :beta2 => beta2,
        :epsilon => epsilon
    )
    
    loss_val, _, _ = train_with_hyperparams(hyperparams, max_iters=100)
    loss_val
end

println("Best ADAM hyperparameters found:")
println("  Learning rate: $(bo_results.minimizer[1])")
println("  β₁: $(bo_results.minimizer[2])")
println("  β₂: $(bo_results.minimizer[3])")
println("  ε: $(bo_results.minimizer[4])")
println("  Best loss: $(bo_results.minimum)")

# -----------------------------------------------------------------------------
# METHOD 2: MULTI-FIDELITY BAYESIAN OPTIMIZATION (Better for expensive problems)

println("\n" * "="^60)
println("METHOD 2: MULTI-FIDELITY BAYESIAN OPTIMIZATION")
println("="^60)
println("Uses cheap approximations (few iterations) to guide expensive evaluations")

function multi_fidelity_objective(params, fidelity)
    """
    Low fidelity: 50 iterations
    High fidelity: 500 iterations
    """
    max_iters = fidelity == :low ? 50 : 500
    
    hyperparams = Dict(
        :optimizer => "ADAM",
        :learning_rate => params[1],
        :beta1 => params[2],
        :beta2 => params[3],
        :epsilon => params[4]
    )
    
    loss_val, _, _ = train_with_hyperparams(hyperparams, max_iters=max_iters)
    return loss_val
end

# Run multi-fidelity optimization
println("Running Multi-fidelity Bayesian Optimization...")

# First, explore with low fidelity
low_fidelity_results = []
for i in 1:15
    params = [
        exp(log(1e-5) + (log(1e-1) - log(1e-5)) * rand()),  # log-uniform
        0.8 + 0.19 * rand(),                                 # uniform
        0.95 + 0.0499 * rand(),                              # uniform
        exp(log(1e-10) + (log(1e-6) - log(1e-10)) * rand()) # log-uniform
    ]
    loss = multi_fidelity_objective(params, :low)
    push!(low_fidelity_results, (params=params, loss=loss))
    println("  Low-fidelity trial $i: Loss = $loss")
end

# Select top 3 configurations for high-fidelity evaluation
sort!(low_fidelity_results, by=x->x.loss)
high_fidelity_results = []
for i in 1:3
    params = low_fidelity_results[i].params
    loss = multi_fidelity_objective(params, :high)
    push!(high_fidelity_results, (params=params, loss=loss))
    println("  High-fidelity trial $i: Loss = $loss")
end

best_mf = high_fidelity_results[argmin([r.loss for r in high_fidelity_results])]
println("Best multi-fidelity hyperparameters: $(best_mf.params)")

# -----------------------------------------------------------------------------
# METHOD 3: HYPERBAND (Successive Halving with Adaptive Resource Allocation)

println("\n" * "="^60)
println("METHOD 3: HYPERBAND - Adaptive Resource Allocation")
println("="^60)
println("Efficiently allocates computational budget using successive halving")

function hyperband_optimization(max_iter=200, η=3)
    """
    Hyperband: Combines random search with adaptive resource allocation
    η: downsampling rate
    """
    R = max_iter  # Maximum iterations per configuration
    s_max = floor(Int, log(R) / log(η))
    B = (s_max + 1) * R  # Total budget
    
    best_config = nothing
    best_loss = Inf
    
    for s in s_max:-1:0
        n = ceil(Int, B / R / (s + 1) * η^s / (η^s))  # Initial configs
        r = R * η^(-s)  # Initial resources
        
        # Generate n random configurations
        configs = []
        for i in 1:n
            config = Dict(
                :optimizer => "ADAM",
                :learning_rate => exp(log(1e-5) + (log(1e-1) - log(1e-5)) * rand()),
                :beta1 => 0.8 + 0.19 * rand(),
                :beta2 => 0.95 + 0.0499 * rand(),
                :epsilon => exp(log(1e-10) + (log(1e-6) - log(1e-10)) * rand())
            )
            push!(configs, config)
        end
        
        # Successive halving
        for i in 0:s
            n_i = floor(Int, n * η^(-i))
            r_i = floor(Int, r * η^i)
            
            # Train all configurations with r_i resources
            results = []
            for config in configs[1:min(n_i, length(configs))]
                loss_val, _, _ = train_with_hyperparams(config, max_iters=r_i, verbose=false)
                push!(results, (config=config, loss=loss_val))
            end
            
            # Keep best n_i/η configurations
            sort!(results, by=x->x.loss)
            configs = [r.config for r in results[1:floor(Int, n_i/η)]]
            
            # Update best
            if length(results) > 0 && results[1].loss < best_loss
                best_loss = results[1].loss
                best_config = results[1].config
            end
            
            println("  Bracket s=$s, round=$i: best loss = $best_loss")
        end
    end
    
    return best_config, best_loss
end

println("Running Hyperband optimization...")
hb_config, hb_loss = @time hyperband_optimization(200, 3)
println("Best Hyperband configuration: $hb_config")
println("Best loss: $hb_loss")

# -----------------------------------------------------------------------------
# METHOD 4: OPTUNA-STYLE (Tree-structured Parzen Estimator with Pruning)

println("\n" * "="^60)
println("METHOD 4: OPTUNA-STYLE TPE WITH PRUNING")
println("="^60)
println("Combines Bayesian optimization with early stopping")

mutable struct MedianPruner
    n_startup_trials::Int
    n_warmup_steps::Int
    interval_steps::Int
end

function should_prune(pruner::MedianPruner, trial_losses::Vector{Float64}, 
                      all_trials_losses::Vector{Vector{Float64}}, step::Int)
    """
    Prune if current trial is worse than median of previous trials at same step
    """
    if length(all_trials_losses) < pruner.n_startup_trials
        return false
    end
    
    if step < pruner.n_warmup_steps
        return false
    end
    
    if step % pruner.interval_steps != 0
        return false
    end
    
    # Get losses at current step from completed trials
    losses_at_step = Float64[]
    for losses in all_trials_losses
        if length(losses) > step
            push!(losses_at_step, losses[step])
        end
    end
    
    if length(losses_at_step) == 0
        return false
    end
    
    # Prune if worse than median
    return trial_losses[end] > median(losses_at_step)
end

function optuna_style_optimization(n_trials=20)
    pruner = MedianPruner(5, 10, 10)  # Start pruning after 5 trials, 10 warmup steps
    all_trials_losses = Vector{Vector{Float64}}[]
    best_config = nothing
    best_loss = Inf
    
    for trial in 1:n_trials
        # Sample hyperparameters using TPE-like strategy
        # (simplified - real TPE uses kernel density estimation)
        if trial <= 5
            # Random sampling for first trials
            config = Dict(
                :optimizer => "ADAM",
                :learning_rate => exp(log(1e-5) + (log(1e-1) - log(1e-5)) * rand()),
                :beta1 => 0.8 + 0.19 * rand(),
                :beta2 => 0.95 + 0.0499 * rand(),
                :epsilon => exp(log(1e-10) + (log(1e-6) - log(1e-10)) * rand())
            )
        else
            # Sample from good/bad regions (simplified TPE)
            # In practice, this would use Gaussian Mixture Models
            sorted_trials = sort(collect(zip(all_trials_losses, 1:length(all_trials_losses))), 
                               by=x->minimum(x[1]))
            good_idx = sorted_trials[1:div(length(sorted_trials), 3)][end][2]
            
            # Perturb best configuration
            base_config = all_trials_losses[good_idx]
            config = Dict(
                :optimizer => "ADAM",
                :learning_rate => exp(log(1e-5) + (log(1e-1) - log(1e-5)) * rand()) * 0.5,
                :beta1 => 0.8 + 0.19 * rand() * 0.5,
                :beta2 => 0.95 + 0.0499 * rand() * 0.5,
                :epsilon => exp(log(1e-10) + (log(1e-6) - log(1e-10)) * rand()) * 0.5
            )
        end
        
        # Train with early stopping
        p_train = copy(p_init)
        trial_losses = Float64[]
        
        callback = function (p, l)
            push!(trial_losses, l)
            
            # Check for pruning
            if should_prune(pruner, trial_losses, all_trials_losses, length(trial_losses))
                println("  Trial $trial pruned at step $(length(trial_losses))")
                return true  # Stop training
            end
            return false
        end
        
        adtype = Optimization.AutoZygote()
        optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
        optprob = Optimization.OptimizationProblem(optf, p_train)
        
        opt = Optimisers.Adam(
            config[:learning_rate],
            (config[:beta1], config[:beta2]),
            config[:epsilon]
        )
        
        res = Optimization.solve(optprob, opt, callback = callback, maxiters = 200)
        
        push!(all_trials_losses, trial_losses)
        
        if length(trial_losses) > 0 && minimum(trial_losses) < best_loss
            best_loss = minimum(trial_losses)
            best_config = config
        end
        
        println("Trial $trial: Loss = $(minimum(trial_losses))")
    end
    
    return best_config, best_loss
end

println("Running Optuna-style optimization with pruning...")
optuna_config, optuna_loss = @time optuna_style_optimization(15)
println("Best Optuna configuration: $optuna_config")
println("Best loss: $optuna_loss")

# -----------------------------------------------------------------------------
# METHOD 5: POPULATION-BASED TRAINING (PBT) - Advanced method

println("\n" * "="^60)
println("METHOD 5: POPULATION-BASED TRAINING (PBT)")
println("="^60)
println("Combines parallel training with online hyperparameter adaptation")

function population_based_training(population_size=8, generations=10, steps_per_gen=25)
    """
    PBT: Trains population in parallel, periodically updates hyperparameters
    """
    # Initialize population
    population = []
    for i in 1:population_size
        member = Dict(
            :id => i,
            :config => Dict(
                :optimizer => "ADAM",
                :learning_rate => exp(log(1e-5) + (log(1e-1) - log(1e-5)) * rand()),
                :beta1 => 0.8 + 0.19 * rand(),
                :beta2 => 0.95 + 0.0499 * rand(),
                :epsilon => exp(log(1e-10) + (log(1e-6) - log(1e-10)) * rand())
            ),
            :params => copy(p_init),
            :loss => Inf,
            :losses => Float64[]
        )
        push!(population, member)
    end
    
    for gen in 1:generations
        println("Generation $gen/$generations")
        
        # Train all members for steps_per_gen iterations
        for member in population
            loss_val, new_params, losses = train_with_hyperparams(
                member[:config], 
                max_iters=steps_per_gen,
                verbose=false
            )
            member[:params] = new_params
            member[:loss] = loss_val
            append!(member[:losses], losses)
        end
        
        # Sort by performance
        sort!(population, by=x->x[:loss])
        
        # Exploit and explore: bottom 50% copy from top 50% with mutation
        n_exploit = div(population_size, 2)
        for i in (n_exploit+1):population_size
            # Copy from top performer
            source_idx = rand(1:n_exploit)
            population[i][:params] = copy(population[source_idx][:params])
            
            # Mutate hyperparameters
            population[i][:config][:learning_rate] *= exp(0.2 * randn())
            population[i][:config][:beta1] = clamp(population[i][:config][:beta1] + 0.05 * randn(), 0.8, 0.99)
            population[i][:config][:beta2] = clamp(population[i][:config][:beta2] + 0.01 * randn(), 0.95, 0.9999)
        end
        
        println("  Best loss: $(population[1][:loss])")
        println("  Best config: $(population[1][:config])")
    end
    
    return population[1][:config], population[1][:loss], population[1][:params]
end

println("Running Population-Based Training...")
pbt_config, pbt_loss, pbt_params = @time population_based_training(6, 5, 40)
println("Best PBT configuration: $pbt_config")
println("Best loss: $pbt_loss")

# -----------------------------------------------------------------------------
# COMPARISON AND RECOMMENDATIONS

println("\n" * "="^60)
println("HYPERPARAMETER OPTIMIZATION RESULTS SUMMARY")
println("="^60)

results_df = DataFrame(
    Method = ["Bayesian Opt", "Multi-Fidelity BO", "Hyperband", "Optuna-style", "PBT"],
    BestLoss = [
        bo_results.minimum,
        best_mf.loss,
        hb_loss,
        optuna_loss,
        pbt_loss
    ],
    Pros = [
        "Efficient exploration, uncertainty quantification",
        "Saves compute with low-fidelity approximations",
        "Adaptive resource allocation, no waste",
        "Early stopping saves time",
        "Online adaptation, parallelizable"
    ],
    Cons = [
        "Sequential, can be slow",
        "Requires good low-fidelity proxy",
        "Many hyperparameters to set",
        "Pruning can be aggressive",
        "Complex implementation"
    ]
)

println(results_df)

# Save best hyperparameters to file
best_hyperparams = Dict(
    "Bayesian" => bo_results.minimizer,
    "Hyperband" => hb_config,
    "Optuna" => optuna_config,
    "PBT" => pbt_config
)

open(joinpath(plots_dir, "best_hyperparameters.json"), "w") do io
    JSON3.write(io, best_hyperparams)
end

println("\n" * "="^60)
println("RECOMMENDATIONS FOR YOUR UDE")
println("="^60)
println("1. For INITIAL EXPLORATION: Use Hyperband or Random Search")
println("   - Fast, parallel, good coverage")
println("2. For FINE-TUNING: Use Bayesian Optimization")
println("   - Efficient when you have good bounds")
println("3. For PRODUCTION: Use Population-Based Training")
println("   - Best final performance, adaptive")
println("4. For LIMITED BUDGET: Use Multi-Fidelity BO")
println("   - Maximizes information per compute")
println("="^60)

# Plot convergence comparison
pl_convergence = plot(title="Hyperparameter Optimization Methods",
                     xlabel="Evaluations",
                     ylabel="Best Loss Found",
                     yaxis=:log10)

# Add convergence curves for each method
# (simplified visualization - in practice track during optimization)
plot!(pl_convergence, [bo_results.minimum], label="Bayesian Opt", marker=:circle)
plot!(pl_convergence, [hb_loss], label="Hyperband", marker=:square)
plot!(pl_convergence, [optuna_loss], label="Optuna", marker=:diamond)
plot!(pl_convergence, [pbt_loss], label="PBT", marker=:star)

savefig(pl_convergence, joinpath(plots_dir, "hyperopt_comparison.pdf"))
display(pl_convergence)
````

## **Key Insights and Recommendations**

### **When to Use Each Method:**

1. **Bayesian Optimization**: 
   - ✅ When training is very expensive (>10 min per run)
   - ✅ Smooth hyperparameter landscape
   - ❌ Not for discrete/categorical parameters

2. **Hyperband**:
   - ✅ When you can evaluate partial training (early stopping)
   - ✅ Large hyperparameter space
   - ❌ Not when early performance doesn't predict final

3. **Population-Based Training (PBT)**:
   - ✅ When you have parallel compute resources
   - ✅ Want to adapt hyperparameters during training
   - ❌ Complex to implement correctly

4. **Multi-Fidelity BO**:
   - ✅ When low-fidelity approximations correlate with high-fidelity
   - ✅ Very expensive full evaluations
   - ❌ Poor low-fidelity proxies

### **For Your UDE Specifically:**

```julia
# Recommended approach
1. Start with Hyperband to explore broadly (1-2 hours)
2. Refine with Bayesian Optimization (2-4 hours)
3. Final training with best hyperparameters (full dataset)
```

The key advantage is that these methods find better hyperparameters with **10-100x fewer evaluations** than grid/random search, which is crucial for expensive UDE training.
~~~

### User
~~~md
<attachment id="file:optimization_Copilot.jl" isSummarized="true">
User's active file:
```julia
# filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\optimization_Copilot.jl
# ---------------------------------------------------------------------------
# Clamped Pinned Rod UDE with Modern Global Optimizers
# 
# Implements better alternatives to ADAM with fewer hyperparameters
# ---------------------------------------------------------------------------
# Initialization
    # Setup project environment and configuration
    include("../utils/project_utils.jl")

    # Setup project environment
    project_root = setup_project_environment(activate_env = true, instantiate = false)

    # Create output directories for plots and results
    plots_dir = joinpath(project_root, "src", "Data", "NODE_Oscar_1MLPx3_GlobalOpt", "Plots_Results")
    mkpath(plots_dir)
    println("Plots & Results will be saved to: ", plots_dir)

    # This script is designed to run on a single thread Julia environment
    println("Running with $(Threads.nthreads()) threads")
    @assert Threads.nthreads() == 1 "Julia must be started with one threads!"

# -----------------------------------------------------------------------------
# Load Packages
    using Lux
    using Optimisers
    using LaTeXStrings
    using Plots
    using BenchmarkTools
    gr()

    # SciML Tools
    using OrdinaryDiffEq, SciMLSensitivity
    using Optimization, OptimizationOptimisers, OptimizationOptimJL

    # Global optimization packages
    using Evolutionary  # For Differential Evolution, PSO, CMA-ES
    using BlackBoxOptim # Alternative black-box optimization
    using Flux         # For additional optimizers like AdaBelief

    # Standard Libraries
    using LinearAlgebra, Statistics
    using Random

    # External Libraries
    using ComponentArrays, Lux, Zygote, StableRNGs
    using CpuId

    # ---------------------------------------------------------------------------
    # Print System Information
    println("\n=== System Information ===")
    begin
        println("Number of Julia threads: ", Threads.nthreads())
    end

# -----------------------------------------------------------------------------
# A.- GENERATING Learning Data from the Ground-Truth data in MATLAB file

# ----------------------------------------------------------------------------
# B.- TRAINING of the UODE with Modern Global Optimizers
    # ...existing code... (rest of the prediction and visualization code remains the same)
```
</attachment>
~~~

### User
~~~md
{"image_url":{"url":"data:image/png;base64,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","detail":"high"},"type":"image_url"}
~~~

### User
~~~md
Can you provide Hyperband and Bayesian optimization code for tuning the hyperparameters of the following options?

# Option 1: Pure gradient-based (fastest)
RAdam (200-500 iter) → LBFGS/LM (to convergence)

# Option 2: Gradient-free exploration (most robust)
CMA-ES (100 iter) → LBFGS/LM (to convergence)

# Option 3: Hybrid (best of both)
DE/PSO (50 iter) → RAdam (200 iter) → LBFGS/LM (to convergence)
~~~

### Assistant
~~~md
I'll provide comprehensive Hyperband and Bayesian Optimization implementations for tuning hyperparameters across all three optimization strategies. This will help you find the optimal configuration for your UDE problem.

````julia
// filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_HyperOpt_Strategies.jl
# ---------------------------------------------------------------------------
# Hyperparameter Optimization for Multi-Stage UDE Training Strategies
# 
# Implements Hyperband and Bayesian Optimization for:
# 1. RAdam → LBFGS/LM
# 2. CMA-ES → LBFGS/LM  
# 3. DE/PSO → RAdam → LBFGS/LM
# ---------------------------------------------------------------------------

# Setup project environment
include("../utils/project_utils.jl")
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories
plots_dir = joinpath(project_root, "src", "Data", "NODE_HyperOpt_Strategies", "Plots_Results")
mkpath(plots_dir)
println("Results will be saved to: ", plots_dir)

# -----------------------------------------------------------------------------
# Load Packages
using Lux
using Optimisers
using LaTeXStrings
using Plots
using BenchmarkTools
gr()

# SciML Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL
using LeastSquaresOptim  # For LM

# Global optimization
using Evolutionary
using BlackBoxOptim

# Hyperparameter optimization
using Hyperopt
using BayesianOptimization
using GaussianProcesses  # For Bayesian Optimization

# Utilities
using LinearAlgebra, Statistics, Random
using ComponentArrays, Lux, Zygote, StableRNGs
using DataFrames, CSV
using JSON3
using ProgressMeter

# -----------------------------------------------------------------------------
# Load your UDE setup (using your existing code)
println("Loading dataset and setting up UDE...")

using MATLAB
filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
mf_DataSet = MatFile(filename_DataSet)
data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")
DataSet = jarray(data_DataSet)

# Use subset for hyperopt speed
num_trajectories = size(DataSet, 1)
trajectories_to_use = min(10, num_trajectories)  # Adjust based on computational budget

# Initialize variables and load data (abbreviated)
θ₀_true, κ₀_true = [], []
px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]

ii = trajectories_to_use
for i in 1:ii
    θ₀_i  = Float32.(DataSet[i,3])
    κ₀_i  = Float32.(DataSet[i,112])
    push!(θ₀_true,  θ₀_i)
    push!(κ₀_true,  κ₀_i)
    
    pX_i = Float32.(DataSet[i,12:61])
    pY_i = Float32.(DataSet[i,62:111])
    θ_i  = Float32.(DataSet[i,162:211])
    κ_i  = Float32.(DataSet[i,112:161])
    push!(px_true, pX_i)
    push!(py_true, pY_i)
    push!(θ_true,  θ_i)
    push!(κ_true,  κ_i)
end

X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]

# Setup neural network
rng = StableRNG(1111)
m, n = 4, 3
layers = [m, 20, 20, 20, n]

const U = Lux.Chain(
    [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
    Dense(layers[end-1] => layers[end], identity),
)

p_init, st = Lux.setup(rng, U)
p_init = ComponentArray{Float32}(p_init)
const _st = st

# Define ODE and prediction functions
function ude_dynamics!(du, u, p, s)
    if any(!isfinite, u)
        du .= 0.0f0
        return nothing
    end
    û_1 = U(u, p, _st)[1]
    du[1] = û_1[1]
    du[2] = û_1[2]
    du[3] = u[4]
    du[4] = û_1[3]
    return nothing
end

sSpan = (0.0f0, 1.0f0)
s = Float32.(range(0, 1, length=50))
u0 = Float32.(X_ssol_all[1][:, 1])
const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p_init)

function predict(θ, X_all = X_ssol_all, S = s)
    try
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
    catch e
        return nothing
    end
end

function loss(θ)
    X̂_sols = predict(θ)
    if X̂_sols === nothing
        return 1e10
    end
    total_loss = sum(mean(abs, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols))
    return total_loss / length(X̂_sols)
end

# -----------------------------------------------------------------------------
# STRATEGY IMPLEMENTATIONS

"""
    train_strategy_1(hyperparams; max_iters_dict)
    
Option 1: RAdam → LBFGS/LM
Hyperparameters:
- radam_lr: learning rate for RAdam
- radam_iters: iterations for RAdam  
- lbfgs_m: memory size for LBFGS
- use_lm: whether to use LM instead of LBFGS
"""
function train_strategy_1(hyperparams; max_iters_dict=Dict(:radam=>200, :refine=>500))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: RAdam
    println("  Strategy 1 - Phase 1: RAdam")
    losses_radam = Float64[]
    callback = function (p, l)
        push!(losses_radam, l)
        push!(all_losses, l)
        return false
    end
    
    adtype = Optimization.AutoZygote()
    optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_current)
    
    t1 = @elapsed begin
        res_radam = Optimization.solve(optprob,
                                       Optimisers.RAdam(hyperparams[:radam_lr]),
                                       callback = callback,
                                       maxiters = hyperparams[:radam_iters])
    end
    total_time += t1
    p_current = res_radam.u
    
    # Phase 2: LBFGS or LM
    if hyperparams[:use_lm]
        println("  Strategy 1 - Phase 2: Levenberg-Marquardt")
        
        # Residual function for LM
        function residual_vector!(resid, θ)
            X̂_sols = predict(θ)
            if X̂_sols === nothing
                resid .= 1e10
                return resid
            end
            idx = 1
            for i in 1:length(X̂_sols)
                for j in 1:size(X_ssol_all[i], 1)
                    for k in 1:size(X_ssol_all[i], 2)
                        resid[idx] = X_ssol_all[i][j, k] - X̂_sols[i][j, k]
                        idx += 1
                    end
                end
            end
            return resid
        end
        
        total_residuals = sum(length(vec(X)) for X in X_ssol_all)
        
        t2 = @elapsed begin
            result = LeastSquaresOptim.optimize!(
                (resid, θ_vec) -> residual_vector!(resid, ComponentArray{Float32}(θ_vec, axes(p_current))),
                zeros(Float32, total_residuals),
                vec(p_current),
                LevenbergMarquardt();
                x_tol = hyperparams[:lm_xtol],
                f_tol = hyperparams[:lm_ftol],
                iterations = max_iters_dict[:refine],
                show_trace = false
            )
            p_current = ComponentArray{Float32}(result.minimizer, axes(p_current))
        end
    else
        println("  Strategy 1 - Phase 2: LBFGS")
        optprob_refine = Optimization.OptimizationProblem(optf, p_current)
        
        t2 = @elapsed begin
            res_lbfgs = Optimization.solve(optprob_refine,
                                          LBFGS(m = hyperparams[:lbfgs_m]),
                                          maxiters = max_iters_dict[:refine])
        end
        p_current = res_lbfgs.u
    end
    total_time += t2
    
    final_loss = loss(p_current)
    push!(all_losses, final_loss)
    
    return final_loss, p_current, all_losses, total_time
end

"""
    train_strategy_2(hyperparams; max_iters_dict)
    
Option 2: CMA-ES → LBFGS/LM
Hyperparameters:
- cmaes_popsize: population size (or auto)
- cmaes_iters: iterations for CMA-ES
- use_lm: whether to use LM instead of LBFGS
- lbfgs_m: memory size for LBFGS
"""
function train_strategy_2(hyperparams; max_iters_dict=Dict(:cmaes=>100, :refine=>500))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: CMA-ES
    println("  Strategy 2 - Phase 1: CMA-ES")
    
    function loss_vec(θ_vec)
        θ_comp = ComponentArray{Float32}(θ_vec, axes(p_current))
        return loss(θ_comp)
    end
    
    lower_bounds = fill(-5.0, length(vec(p_current)))
    upper_bounds = fill(5.0, length(vec(p_current)))
    
    n_params = length(vec(p_current))
    if hyperparams[:cmaes_popsize] == "auto"
        μ = div(n_params, 4)
        λ = 2 * μ
    else
        λ = hyperparams[:cmaes_popsize]
        μ = div(λ, 2)
    end
    
    t1 = @elapsed begin
        result_cmaes = Evolutionary.optimize(
            loss_vec,
            BoxConstraints(lower_bounds, upper_bounds),
            CMAES(μ = μ, λ = λ),
            Evolutionary.Options(
                iterations = hyperparams[:cmaes_iters],
                show_trace = false,
                store_trace = true
            )
        )
        p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_cmaes), axes(p_current))
        
        # Extract losses from trace
        for state in result_cmaes.trace
            push!(all_losses, state.value)
        end
    end
    total_time += t1
    
    # Phase 2: LBFGS or LM (same as Strategy 1)
    if hyperparams[:use_lm]
        println("  Strategy 2 - Phase 2: Levenberg-Marquardt")
        # [LM code same as Strategy 1]
        function residual_vector!(resid, θ)
            X̂_sols = predict(θ)
            if X̂_sols === nothing
                resid .= 1e10
                return resid
            end
            idx = 1
            for i in 1:length(X̂_sols)
                for j in 1:size(X_ssol_all[i], 1)
                    for k in 1:size(X_ssol_all[i], 2)
                        resid[idx] = X_ssol_all[i][j, k] - X̂_sols[i][j, k]
                        idx += 1
                    end
                end
            end
            return resid
        end
        
        total_residuals = sum(length(vec(X)) for X in X_ssol_all)
        
        t2 = @elapsed begin
            result = LeastSquaresOptim.optimize!(
                (resid, θ_vec) -> residual_vector!(resid, ComponentArray{Float32}(θ_vec, axes(p_current))),
                zeros(Float32, total_residuals),
                vec(p_current),
                LevenbergMarquardt();
                x_tol = hyperparams[:lm_xtol],
                f_tol = hyperparams[:lm_ftol],
                iterations = max_iters_dict[:refine],
                show_trace = false
            )
            p_current = ComponentArray{Float32}(result.minimizer, axes(p_current))
        end
    else
        println("  Strategy 2 - Phase 2: LBFGS")
        adtype = Optimization.AutoZygote()
        optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
        optprob_refine = Optimization.OptimizationProblem(optf, p_current)
        
        t2 = @elapsed begin
            res_lbfgs = Optimization.solve(optprob_refine,
                                          LBFGS(m = hyperparams[:lbfgs_m]),
                                          maxiters = max_iters_dict[:refine])
            p_current = res_lbfgs.u
        end
    end
    total_time += t2
    
    final_loss = loss(p_current)
    push!(all_losses, final_loss)
    
    return final_loss, p_current, all_losses, total_time
end

"""
    train_strategy_3(hyperparams; max_iters_dict)
    
Option 3: DE/PSO → RAdam → LBFGS/LM
Hyperparameters:
- use_de: true for DE, false for PSO
- de_popsize: population size for DE
- de_f: mutation factor
- de_cr: crossover rate
- pso_particles: number of particles
- pso_w: inertia weight
- global_iters: iterations for DE/PSO
- radam_lr: learning rate for RAdam
- radam_iters: iterations for RAdam
- use_lm: whether to use LM
"""
function train_strategy_3(hyperparams; max_iters_dict=Dict(:global=>50, :radam=>200, :refine=>500))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: DE or PSO
    function loss_vec(θ_vec)
        θ_comp = ComponentArray{Float32}(θ_vec, axes(p_current))
        return loss(θ_comp)
    end
    
    lower_bounds = fill(-5.0, length(vec(p_current)))
    upper_bounds = fill(5.0, length(vec(p_current)))
    
    if hyperparams[:use_de]
        println("  Strategy 3 - Phase 1: Differential Evolution")
        t1 = @elapsed begin
            result_de = Evolutionary.optimize(
                loss_vec,
                BoxConstraints(lower_bounds, upper_bounds),
                DE(populationSize = hyperparams[:de_popsize],
                   F = hyperparams[:de_f],
                   CR = hyperparams[:de_cr]),
                Evolutionary.Options(
                    iterations = hyperparams[:global_iters],
                    show_trace = false,
                    store_trace = true
                )
            )
            p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_de), axes(p_current))
            for state in result_de.trace
                push!(all_losses, state.value)
            end
        end
    else
        println("  Strategy 3 - Phase 1: Particle Swarm")
        t1 = @elapsed begin
            result_pso = Evolutionary.optimize(
                loss_vec,
                BoxConstraints(lower_bounds, upper_bounds),
                PSO(n_particles = hyperparams[:pso_particles],
                    w = hyperparams[:pso_w],
                    c1 = hyperparams[:pso_c1],
                    c2 = hyperparams[:pso_c2]),
                Evolutionary.Options(
                    iterations = hyperparams[:global_iters],
                    show_trace = false,
                    store_trace = true
                )
            )
            p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_pso), axes(p_current))
            for state in result_pso.trace
                push!(all_losses, state.value)
            end
        end
    end
    total_time += t1
    
    # Phase 2: RAdam
    println("  Strategy 3 - Phase 2: RAdam")
    adtype = Optimization.AutoZygote()
    optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_current)
    
    losses_radam = Float64[]
    callback = function (p, l)
        push!(losses_radam, l)
        push!(all_losses, l)
        return false
    end
    
    t2 = @elapsed begin
        res_radam = Optimization.solve(optprob,
                                       Optimisers.RAdam(hyperparams[:radam_lr]),
                                       callback = callback,
                                       maxiters = hyperparams[:radam_iters])
    end
    total_time += t2
    p_current = res_radam.u
    
    # Phase 3: LBFGS or LM (same as previous strategies)
    if hyperparams[:use_lm]
        println("  Strategy 3 - Phase 3: Levenberg-Marquardt")
        # [LM code same as before]
        function residual_vector!(resid, θ)
            X̂_sols = predict(θ)
            if X̂_sols === nothing
                resid .= 1e10
                return resid
            end
            idx = 1
            for i in 1:length(X̂_sols)
                for j in 1:size(X_ssol_all[i], 1)
                    for k in 1:size(X_ssol_all[i], 2)
                        resid[idx] = X_ssol_all[i][j, k] - X̂_sols[i][j, k]
                        idx += 1
                    end
                end
            end
            return resid
        end
        
        total_residuals = sum(length(vec(X)) for X in X_ssol_all)
        
        t3 = @elapsed begin
            result = LeastSquaresOptim.optimize!(
                (resid, θ_vec) -> residual_vector!(resid, ComponentArray{Float32}(θ_vec, axes(p_current))),
                zeros(Float32, total_residuals),
                vec(p_current),
                LevenbergMarquardt();
                x_tol = hyperparams[:lm_xtol],
                f_tol = hyperparams[:lm_ftol],
                iterations = max_iters_dict[:refine],
                show_trace = false
            )
            p_current = ComponentArray{Float32}(result.minimizer, axes(p_current))
        end
    else
        println("  Strategy 3 - Phase 3: LBFGS")
        optprob_refine = Optimization.OptimizationProblem(optf, p_current)
        
        t3 = @elapsed begin
            res_lbfgs = Optimization.solve(optprob_refine,
                                          LBFGS(m = hyperparams[:lbfgs_m]),
                                          maxiters = max_iters_dict[:refine])
            p_current = res_lbfgs.u
        end
    end
    total_time += t3
    
    final_loss = loss(p_current)
    push!(all_losses, final_loss)
    
    return final_loss, p_current, all_losses, total_time
end

# -----------------------------------------------------------------------------
# HYPERBAND IMPLEMENTATION

println("\n" * "="^60)
println("HYPERBAND OPTIMIZATION FOR ALL STRATEGIES")
println("="^60)

function hyperband_for_strategy(strategy_fn, hyperspace, strategy_name; 
                                max_resource=400, η=3)
    """
    Hyperband for a given strategy
    η: downsampling factor (typically 3)
    max_resource: maximum iterations to allocate
    """
    
    R = max_resource
    s_max = floor(Int, log(R) / log(η))
    B = (s_max + 1) * R
    
    best_config = nothing
    best_loss = Inf
    all_results = []
    
    println("Running Hyperband for $strategy_name")
    println("s_max = $s_max, B = $B")
    
    for s in s_max:-1:0
        println("\nBracket s = $s")
        n = ceil(Int, B / R / (s + 1) * η^s / (η^s))
        r = R * η^(-s)
        
        println("Initial configs: n = $n, initial resource: r = $r")
        
        # Generate n random configurations
        configs = []
        for i in 1:n
            config = Dict{Symbol, Any}()
            for (param_name, param_range) in hyperspace
                if param_range isa Tuple{Float64, Float64}
                    # Continuous parameter
                    config[param_name] = param_range[1] + (param_range[2] - param_range[1]) * rand()
                elseif param_range isa Tuple{Int, Int}
                    # Integer parameter
                    config[param_name] = rand(param_range[1]:param_range[2])
                elseif param_range isa Vector
                    # Categorical parameter
                    config[param_name] = rand(param_range)
                elseif param_range isa Tuple{String, String} && param_range[1] == "log"
                    # Log-scale parameter
                    low, high = log(parse(Float64, split(param_range[2], ":")[1])), 
                               log(parse(Float64, split(param_range[2], ":")[2]))
                    config[param_name] = exp(low + (high - low) * rand())
                end
            end
            push!(configs, config)
        end
        
        # Successive halving
        for i in 0:s
            n_i = floor(Int, n * η^(-i))
            r_i = floor(Int, r * η^i)
            
            println("  Round $i: evaluating $(min(n_i, length(configs))) configs with resource $r_i")
            
            # Evaluate all configurations with r_i resources
            results = []
            for (idx, config) in enumerate(configs[1:min(n_i, length(configs))])
                # Adjust iterations based on resource allocation
                if haskey(config, :radam_iters)
                    config[:radam_iters] = min(config[:radam_iters], r_i)
                end
                if haskey(config, :cmaes_iters)
                    config[:cmaes_iters] = min(config[:cmaes_iters], r_i)
                end
                if haskey(config, :global_iters)
                    config[:global_iters] = min(config[:global_iters], div(r_i, 3))
                    config[:radam_iters] = min(config[:radam_iters], div(r_i, 2))
                end
                
                try
                    loss_val, _, _, time = strategy_fn(config)
                    push!(results, (config=config, loss=loss_val, time=time))
                    println("    Config $idx: loss = $(round(loss_val, digits=6))")
                catch e
                    println("    Config $idx failed: $e")
                    push!(results, (config=config, loss=1e10, time=0.0))
                end
            end
            
            # Keep best n_i/η configurations
            sort!(results, by=x->x.loss)
            n_keep = max(1, floor(Int, n_i/η))
            configs = [r.config for r in results[1:min(n_keep, length(results))]]
            
            # Track best
            if length(results) > 0 && results[1].loss < best_loss
                best_loss = results[1].loss
                best_config = results[1].config
            end
            
            push!(all_results, results)
        end
    end
    
    return best_config, best_loss, all_results
end

# Define hyperparameter spaces for each strategy
hyperspace_strategy1 = [
    :radam_lr => ("log", "1e-5:1e-1"),
    :radam_iters => (100, 500),
    :use_lm => [true, false],
    :lbfgs_m => (5, 20),
    :lm_xtol => ("log", "1e-10:1e-6"),
    :lm_ftol => ("log", "1e-10:1e-6")
]

hyperspace_strategy2 = [
    :cmaes_popsize => ["auto", 20, 50, 100],
    :cmaes_iters => (50, 200),
    :use_lm => [true, false],
    :lbfgs_m => (5, 20),
    :lm_xtol => ("log", "1e-10:1e-6"),
    :lm_ftol => ("log", "1e-10:1e-6")
]

hyperspace_strategy3 = [
    :use_de => [true, false],
    :de_popsize => (20, 100),
    :de_f => (0.5, 1.0),
    :de_cr => (0.7, 1.0),
    :pso_particles => (20, 50),
    :pso_w => (0.4, 0.9),
    :pso_c1 => (1.0, 2.0),
    :pso_c2 => (1.0, 2.0),
    :global_iters => (30, 100),
    :radam_lr => ("log", "1e-5:1e-1"),
    :radam_iters => (100, 300),
    :use_lm => [true, false],
    :lbfgs_m => (5, 20),
    :lm_xtol => ("log", "1e-10:1e-6"),
    :lm_ftol => ("log", "1e-10:1e-6")
]

# Run Hyperband for each strategy
hyperband_results = Dict()

println("\n" * "="^30 * " STRATEGY 1 " * "="^30)
best_config1, best_loss1, results1 = hyperband_for_strategy(
    train_strategy_1, hyperspace_strategy1, "Strategy 1: RAdam → LBFGS/LM",
    max_resource=300, η=3
)
hyperband_results["Strategy1"] = (config=best_config1, loss=best_loss1)

println("\n" * "="^30 * " STRATEGY 2 " * "="^30)
best_config2, best_loss2, results2 = hyperband_for_strategy(
    train_strategy_2, hyperspace_strategy2, "Strategy 2: CMA-ES → LBFGS/LM",
    max_resource=250, η=3
)
hyperband_results["Strategy2"] = (config=best_config2, loss=best_loss2)

println("\n" * "="^30 * " STRATEGY 3 " * "="^30)
best_config3, best_loss3, results3 = hyperband_for_strategy(
    train_strategy_3, hyperspace_strategy3, "Strategy 3: DE/PSO → RAdam → LBFGS/LM",
    max_resource=400, η=3
)
hyperband_results["Strategy3"] = (config=best_config3, loss=best_loss3)

# -----------------------------------------------------------------------------
# BAYESIAN OPTIMIZATION IMPLEMENTATION

println("\n" * "="^60)
println("BAYESIAN OPTIMIZATION FOR ALL STRATEGIES")
println("="^60)

using GaussianProcesses
using Optim  # For acquisition function optimization

"""
Bayesian Optimization with Gaussian Process surrogate
"""
function bayesian_optimization_strategy(strategy_fn, bounds_dict, strategy_name;
                                       n_initial=5, n_iterations=20)
    
    println("Running Bayesian Optimization for $strategy_name")
    
    # Convert bounds to arrays
    param_names = collect(keys(bounds_dict))
    lower_bounds = Float64[]
    upper_bounds = Float64[]
    log_scale = Bool[]
    
    for (param, bound) in bounds_dict
        if bound[1] == "log"
            push!(lower_bounds, log(bound[2]))
            push!(upper_bounds, log(bound[3]))
            push!(log_scale, true)
        else
            push!(lower_bounds, bound[1])
            push!(upper_bounds, bound[2])
            push!(log_scale, false)
        end
    end
    
    n_dims = length(param_names)
    
    # Initial random sampling
    X_observed = zeros(n_dims, 0)
    y_observed = Float64[]
    
    println("Initial random sampling ($n_initial points)...")
    for i in 1:n_initial
        # Sample random point
        x = zeros(n_dims)
        for j in 1:n_dims
            x[j] = lower_bounds[j] + (upper_bounds[j] - lower_bounds[j]) * rand()
        end
        
        # Convert to hyperparameters
        config = Dict{Symbol, Any}()
        for (j, param) in enumerate(param_names)
            if log_scale[j]
                config[param] = exp(x[j])
            else
                config[param] = x[j]
            end
            
            # Convert continuous to integer if needed
            if param in [:radam_iters, :cmaes_iters, :global_iters, :lbfgs_m, 
                        :de_popsize, :pso_particles]
                config[param] = round(Int, config[param])
            end
            
            # Convert to boolean if needed
            if param == :use_lm || param == :use_de
                config[param] = config[param] > 0.5
            end
        end
        
        # Evaluate
        try
            loss_val, _, _, _ = strategy_fn(config)
            X_observed = hcat(X_observed, x)
            push!(y_observed, loss_val)
            println("  Sample $i: loss = $(round(loss_val, digits=6))")
        catch e
            println("  Sample $i failed: $e")
            X_observed = hcat(X_observed, x)
            push!(y_observed, 1e10)
        end
    end
    
    # Bayesian optimization loop
    for iter in 1:n_iterations
        println("\nBO Iteration $iter/$n_iterations")
        
        # Fit Gaussian Process
        # Normalize observations for numerical stability
        y_mean = mean(y_observed)
        y_std = std(y_observed) + 1e-6
        y_normalized = (y_observed .- y_mean) ./ y_std
        
        # Create GP with Matern kernel
        kern = Mat52Iso(1.0, 1.0)
        gp = GP(X_observed', y_normalized, MeanConst(0.0), kern, -2.0)
        
        # Optimize GP hyperparameters
        optimize!(gp)
        
        # Acquisition function: Expected Improvement
        best_y = minimum(y_normalized)
        
        function acquisition(x)
            μ, σ² = predict_f(gp, reshape(x, :, 1))
            σ = sqrt(σ²[1] + 1e-6)
            
            # Expected Improvement
            if σ > 0
                Z = (best_y - μ[1]) / σ
                Φ = cdf(Normal(), Z)
                φ = pdf(Normal(), Z)
                ei = σ * (Z * Φ + φ)
                return -ei  # Minimize negative EI
            else
                return 0.0
            end
        end
        
        # Find next point by optimizing acquisition function
        result = optimize(acquisition, lower_bounds, upper_bounds, 
                         lower_bounds .+ (upper_bounds .- lower_bounds) .* rand(n_dims),
                         Fminbox(LBFGS()), 
                         Optim.Options(iterations=100))
        x_next = Optim.minimizer(result)
        
        # Convert to hyperparameters and evaluate
        config = Dict{Symbol, Any}()
        for (j, param) in enumerate(param_names)
            if log_scale[j]
                config[param] = exp(x_next[j])
            else
                config[param] = x_next[j]
            end
            
            # Type conversions
            if param in [:radam_iters, :cmaes_iters, :global_iters, :lbfgs_m,
                        :de_popsize, :pso_particles]
                config[param] = round(Int, config[param])
            end
            if param == :use_lm || param == :use_de
                config[param] = config[param] > 0.5
            end
        end
        
        # Evaluate new configuration
        try
            loss_val, _, _, _ = strategy_fn(config)
            X_observed = hcat(X_observed, x_next)
            push!(y_observed, loss_val)
            println("  New sample: loss = $(round(loss_val, digits=6))")
            
            if loss_val < minimum(y_observed[1:end-1])
                println("  🎯 New best configuration found!")
            end
        catch e
            println("  Evaluation failed: $e")
            X_observed = hcat(X_observed, x_next)
            push!(y_observed, 1e10)
        end
    end
    
    # Return best configuration
    best_idx = argmin(y_observed)
    best_x = X_observed[:, best_idx]
    
    best_config = Dict{Symbol, Any}()
    for (j, param) in enumerate(param_names)
        if log_scale[j]
            best_config[param] = exp(best_x[j])
        else
            best_config[param] = best_x[j]
        end
        
        # Type conversions
        if param in [:radam_iters, :cmaes_iters, :global_iters, :lbfgs_m,
                    :de_popsize, :pso_particles]
            best_config[param] = round(Int, best_config[param])
        end
        if param == :use_lm || param == :use_de
            best_config[param] = best_config[param] > 0.5
        end
    end
    
    return best_config, y_observed[best_idx], X_observed, y_observed
end

# Define bounds for Bayesian Optimization
bo_bounds_strategy1 = Dict(
    :radam_lr => ("log", 1e-5, 1e-1),
    :radam_iters => ("linear", 100.0, 500.0),
    :use_lm => ("linear", 0.0, 1.0),
    :lbfgs_m => ("linear", 5.0, 20.0),
    :lm_xtol => ("log", 1e-10, 1e-6),
    :lm_ftol => ("log", 1e-10, 1e-6)
)

bo_bounds_strategy2 = Dict(
    :cmaes_popsize => ("linear", 20.0, 100.0),
    :cmaes_iters => ("linear", 50.0, 200.0),
    :use_lm => ("linear", 0.0, 1.0),
    :lbfgs_m => ("linear", 5.0, 20.0),
    :lm_xtol => ("log", 1e-10, 1e-6),
    :lm_ftol => ("log", 1e-10, 1e-6)
)

bo_bounds_strategy3 = Dict(
    :use_de => ("linear", 0.0, 1.0),
    :de_popsize => ("linear", 20.0, 100.0),
    :de_f => ("linear", 0.5, 1.0),
    :de_cr => ("linear", 0.7, 1.0),
    :pso_particles => ("linear", 20.0, 50.0),
    :pso_w => ("linear", 0.4, 0.9),
    :global_iters => ("linear", 30.0, 100.0),
    :radam_lr => ("log", 1e-5, 1e-1),
    :radam_iters => ("linear", 100.0, 300.0),
    :use_lm => ("linear", 0.0, 1.0),
    :lbfgs_m => ("linear", 5.0, 20.0)
)

# Run Bayesian Optimization
bo_results = Dict()

println("\n" * "="^30 * " BO STRATEGY 1 " * "="^30)
bo_config1, bo_loss1, _, _ = bayesian_optimization_strategy(
    train_strategy_1, bo_bounds_strategy1, 
    "Strategy 1: RAdam → LBFGS/LM",
    n_initial=5, n_iterations=15
)
bo_results["Strategy1"] = (config=bo_config1, loss=bo_loss1)

println("\n" * "="^30 * " BO STRATEGY 2 " * "="^30)  
bo_config2, bo_loss2, _, _ = bayesian_optimization_strategy(
    train_strategy_2, bo_bounds_strategy2,
    "Strategy 2: CMA-ES → LBFGS/LM", 
    n_initial=5, n_iterations=15
)
bo_results["Strategy2"] = (config=bo_config2, loss=bo_loss2)

println("\n" * "="^30 * " BO STRATEGY 3 " * "="^30)
bo_config3, bo_loss3, _, _ = bayesian_optimization_strategy(
    train_strategy_3, bo_bounds_strategy3,
    "Strategy 3: DE/PSO → RAdam → LBFGS/LM",
    n_initial=5, n_iterations=15  
)
bo_results["Strategy3"] = (config=bo_config3, loss=bo_loss3)

# -----------------------------------------------------------------------------
# FINAL COMPARISON AND RECOMMENDATIONS

println("\n" * "="^60)
println("FINAL RESULTS COMPARISON")
println("="^60)

# Create comparison table
comparison_df = DataFrame(
    Strategy = String[],
    Method = String[],
    BestLoss = Float64[],
    Config = String[]
)

for (strategy_name, result) in hyperband_results
    push!(comparison_df, (strategy_name, "Hyperband", result.loss, 
                          string(result.config)))
end

for (strategy_name, result) in bo_results
    push!(comparison_df, (strategy_name, "Bayesian Opt", result.loss,
                          string(result.config)))
end

sort!(comparison_df, :BestLoss)
println(comparison_df)

# Save best configurations
best_configs = Dict(
    "Hyperband" => hyperband_results,
    "BayesianOpt" => bo_results,
    "Comparison" => comparison_df
)

open(joinpath(plots_dir, "best_hyperparameters_strategies.json"), "w") do io
    JSON3.write(io, best_configs)
end

# Identify overall best
overall_best = comparison_df[1, :]
println("\n" * "="^60)
println("🏆 OVERALL BEST CONFIGURATION")
println("="^60)
println("Strategy: $(overall_best.Strategy)")
println("Method: $(overall_best.Method)")
println("Loss: $(overall_best.BestLoss)")
println("Configuration: $(overall_best.Config)")
println("="^60)

# Plot convergence comparison
pl_comparison = plot(title="Hyperparameter Optimization Results",
                    xlabel="Strategy",
                    ylabel="Best Loss",
                    yaxis=:log10,
                    legend=:topright)

strategies = ["Strategy1", "Strategy2", "Strategy3"]
hb_losses = [hyperband_results[s].loss for s in strategies]
bo_losses = [bo_results[s].loss for s in strategies]

bar!(pl_comparison, strategies, hb_losses, label="Hyperband", alpha=0.7)
bar!(pl_comparison, strategies, bo_losses, label="Bayesian Opt", alpha=0.7)

savefig(pl_comparison, joinpath(plots_dir, "hyperopt_strategies_comparison.pdf"))
display(pl_comparison)

println("\n✅ Hyperparameter optimization complete!")
println("Results saved to: $plots_dir")
````

This implementation provides:

## **Key Features:**

1. **Three Training Strategies:**
   - Strategy 1: RAdam → LBFGS/LM
   - Strategy 2: CMA-ES → LBFGS/LM
   - Strategy 3: DE/PSO → RAdam → LBFGS/LM

2. **Hyperband Implementation:**
   - Adaptive resource allocation
   - Successive halving for efficient search
   - Handles different hyperparameter types (continuous, integer, categorical)

3. **Bayesian Optimization:**
   - Gaussian Process surrogate model
   - Expected Improvement acquisition
   - Automatic hyperparameter type conversion

4. **Comprehensive Hyperparameter Spaces:**
   - Learning rates (log scale)
   - Iteration counts
   - Algorithm-specific parameters
   - Choice between LBFGS and LM

## **Usage Recommendations:**

1. **For Quick Results:** Use Hyperband with η=3, max_resource=200
2. **For Best Performance:** Use Bayesian Optimization with 25-30 iterations
3. **For Production:** Take best config from either method and train on full dataset

The code automatically saves the best configurations and provides detailed comparisons between all strategies and optimization methods.
~~~

### User
~~~md
<attachments>
<attachment id="file:UDE_Rotate_Clamp_FixedTip_Oscar_1MLPx3_Training_generated.jl">
User's active file:
```julia
# filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_Rotate_Clamp_FixedTip_Oscar_1MLPx3_Training_generated.jl
# ---------------------------------------------------------------------------
# Clamped Pinned Rod UDE
# 
# Solves the inverse position problem of a Clamped-pinned Cosserat beam using Neural ODE Networks. UDE methodology is implemented.
# ---------------------------------------------------------------------------

#using Pkg
#Pkg.develop(path=joinpath(homedir(), ".julia", "dev", "ClampedPinnedRodSolver"))
#using ClampedPinnedRodSolver

# Setup project environment and configuration
include("../utils/project_utils.jl")

# Setup project environment
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories for plots and results
plots_dir = joinpath(project_root, "src", "Data", "NODE_Oscar_1MLPx3", "Plots_Results")
mkpath(plots_dir)
println("Plots & Results will be saved to: ", plots_dir)

# This script is designed to run on a single thread Julia environment
println("Running with $(Threads.nthreads()) threads")
@assert Threads.nthreads() == 1 "Julia must be started with one threads!"


# -----------------------------------------------------------------------------
# 0 .- Load Packages
#using ClampedPinnedRodUDE
    using Lux #, LuxCUDA  # The deep learning (neural network) framework. I do not use GPU now.
    using Optimisers
    using LaTeXStrings
    using Plots
    using BenchmarkTools  # Added for timing the training process
    gr()
        # SciML Tools
            using OrdinaryDiffEq, SciMLSensitivity
            using Optimization, OptimizationOptimisers, OptimizationOptimJL, LineSearches

        # Standard Libraries
            using LinearAlgebra, Statistics

        # External Libraries
            using ComponentArrays, Lux, Zygote, StableRNGs

        using CpuId
# ---------------------------------------------------------------------------
# Print System Information
    println("\n=== System Information ===")
    # 
    begin
        cpu_name = CpuId.cpubrand()
        println("CPU: ", cpu_name)
        println("Physical cores: ", CpuId.cpucores())
        println("Logical cores (threads): ", CpuId.cputhreads())
        println("Number of Julia threads: ", Threads.nthreads())
    end
    println("=== System Information ===")
        
# -----------------------------------------------------------------------------
# A.- GENERATING Learning Data from the Ground-Truth data in MATLAB file: (px(θ₀), py(θ₀), κ₀(θ₀), θ₀) 
    
    # Reading .mat file  
        using MATLAB

        # Use relative path based on project root for repository portability
        filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
        println("Loading dataset from: ", filename_DataSet)
        
        # Verify file exists before attempting to load
        if !isfile(filename_DataSet)
            error("Dataset file not found at: $filename_DataSet")
        end
        
        mf_DataSet = MatFile(filename_DataSet)                # opens a MAT file for reading
        data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")   # gets a variable and returns an mxArray
        DataSet = jarray(data_DataSet)                        # converts x to a Julia matrix
        
        # Dynamically determine the number of trajectories from the dataset
        num_trajectories = size(DataSet, 1)
        println("Found $num_trajectories trajectories in the dataset")
        
        # Configuration: You can modify this to use fewer trajectories if needed
        # Set to `nothing` to use all available trajectories, or specify a number
        max_trajectories_to_use = nothing  # Change to a number like 35 if you want to limit
        #max_trajectories_to_use = 10  
        
        # Determine actual number of trajectories to use
        trajectories_to_use = if max_trajectories_to_use === nothing
            num_trajectories
        else
            min(max_trajectories_to_use, num_trajectories)
        end
        
        println("Using $trajectories_to_use out of $num_trajectories available trajectories")
    
    # Initialize variables
    θ₀_true, κ₀_true = [], []
    px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]
        
    # Load θ₀ and κ₀ from data 
        for i in 1:trajectories_to_use # Number of trajectories: dynamically determined
            θ₀_i  = Float32.(DataSet[i,3]);   # input: initial theta angle, convert to Float32.
            κ₀_i  = Float32.(DataSet[i,112]); # input: initial curvature, convert to Float32.
            # Append to lists
            push!(θ₀_true,  θ₀_i)
            push!(κ₀_true,  κ₀_i)
        end
        θ₀_true = Float32.(collect(θ₀_true))
        κ₀_true = Float32.(collect(κ₀_true))
        
    # Load pX(s), pY(s), θ(s) and κ(s) from data to create the Learning set. 
                # (s): 50 discrete values of s in DataSet
        ii = trajectories_to_use # Use the determined number of trajectories
        for i in 1:ii
            pX_i = Float32.(DataSet[i,12:61]  );   # input: position X, convert to Float32.
            pY_i = Float32.(DataSet[i,62:111] );   # input: position Y, convert to Float32.
            θ_i  = Float32.(DataSet[i,162:211]);   # input: theta angle, convert to Float32.
            κ_i  = Float32.(DataSet[i,112:161]);   # input: curvature, convert to Float32.
            # Append to lists
            push!(px_true, pX_i)
            push!(py_true, pY_i)
            push!(θ_true,  θ_i)
            push!(κ_true,  κ_i)
        end

    # Combine all ii trajectories into a single Learning dataset
        X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]
            
# ----------------------------------------------------------------------------
# B.- TRAINING of the UODE

    # Set a random seed for reproducible behaviour
        rng = StableRNG(1111);

    # Definition of the Universal Ordinary Differential Equation
        # Choose number of inputs, outputs
        m = 4
        n = 3
        # Define hyperparameters for the MLP neural network
        layers = collect([m, 20, 20, 20, n])
  
        # Multilayer FeedForward
        const U = Lux.Chain(
            [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
            Dense(layers[end-1] => layers[end], identity),) 
            
            # Get the initial parameters and state variables of the model
            p, st = Lux.setup(rng, U)# network U initialization using random numbers
            p = ComponentArray{Float32}(p)  # Changed from Float64 to Float32
            const _st = st
        # Global counters for NN passes
            const NN_FORWARD_COUNT = Ref(0)
            const NN_BACKWARD_COUNT = Ref(0)
        # Define the ODE
            function ude_dynamics!(du, u, p, s)
                NN_FORWARD_COUNT[] += 1  # Count forward pass
                # Add numerical stability checks
                if any(!isfinite, u)
                    # If any state is non-finite, set derivatives to zero to stop integration
                    du .= 0.0f0
                    return nothing
                end
                # Current state.
                # x, y, θ, κ = u
                # Network prediction (forward pass), used to modify the derivatives of the system´s state: du[1] and du[2].
                û_1 = U(u, p, _st)[1] 
 
                # Evaluate differential equations.
                du[1] = û_1[1]   # du[1] = cos(θ)                                                                   
                du[2] = û_1[2]   # du[2] = sin(θ)                                                                    
                du[3] = u[4]     # du[3] = κ                                                           
                du[4] = û_1[3]   # du[4] = R / (E * I) * sin(θ - Ψ)                                                                 
                return nothing
            end 

    # Sampling & model parameter space
        sSpan = (0.0f0, 1.0f0)
        s = Float32.(range(0, 1, length=50))  # Explicitly create 50 points

    # Use the first trajectory's initial state to define prob_nn
        u0 = Float32.(X_ssol_all[1][:, 1])
        const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p)
    
    # Update the predict function to handle the initial conditions and parameters from the optimizer
    function predict(θ, X_all = X_ssol_all, S = s)
        # Enable experimental debug mode to understand the issue better
        if haskey(ENV, "LUX_DEBUG")
            Lux.Experimental.@debug_mode true
        end
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),    
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,                  
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
            end
    
    # Update the loss function to compute the loss over all trajectories
    function loss(θ)
        X̂_sols = predict(θ)
        total_loss = sum(mean(abs, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols))
        return total_loss / length(X̂_sols)
    end

    losses = Float64[]

    # Callback
        callback = function (p, l)
            push!(losses, l)
            if length(losses) % 50 == 0
                println("Current loss after $(length(losses)) iterations: $(losses[end])")
            end
            return false
        end

    # Training (solving optimization problem)
     # Define the optimizer    

        adtype = Optimization.AutoZygote()                                                 
        optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)               
        optprob = Optimization.OptimizationProblem(optf, ComponentArray{Float32}(p))  # Changed to Float32     
        
        # Benchmark the training process
        println("Starting training benchmark...")
        training_time = @elapsed begin
            # ADAM loop
            learning_rate = 0.001
            epoch_1 = 1000 # ADAM
            println("Starting ADAM optimization ($(epoch_1) iterations)...")
            adam_time = @elapsed (res1 = Optimization.solve(optprob, OptimizationOptimisers.Adam(learning_rate), callback = callback, maxiters = epoch_1))
            
            # LBFGS loop
            epoch_2 = 10000 # LBFGS
            println("Starting LBFGS optimization ($(epoch_2) iterations)...")
            optprob2 = Optimization.OptimizationProblem(optf, res1.u)
            lbfgs_time = @elapsed (res2 = Optimization.solve(optprob2, LBFGS(linesearch = BackTracking()), callback = callback, maxiters = epoch_2))
            
            # Print timing results
            println("\n" * "="^60)
            println("TRAINING BENCHMARK RESULTS")
            println("="^60)
            println("ADAM phase:  $(round(adam_time, digits=2)) seconds ($(epoch_1) iterations)")
            println("LBFGS phase: $(round(lbfgs_time, digits=2)) seconds ($(epoch_2) iterations)")
            println("Total time:  $(round(adam_time + lbfgs_time, digits=2)) seconds")
            println("="^60)
        end

    # Final Losses Plot
        begin
            println("Final training loss after $(length(losses)) iterations: $(losses[end])")
            pl_losses = Plots.plot(1:epoch_1, losses[1:epoch_1], yaxis = :log10, xaxis = :log10,
                                xlabel = "Iterations", ylabel = "Loss", label = "ADAM", color = :blue)
            Plots.plot!(epoch_1 + 1:length(losses), losses[epoch_1 + 1:end], yaxis = :log10, xaxis = :log10,
                        xlabel = "Iterations", ylabel = "Loss", label = "LBFGS", color = :red)
            #Plots.savefig(pl_losses, "theta_0_sol_1_mode_2_X02_losses.pdf")
            display(pl_losses)
        end

    # Rename the best candidate
        p_trained = res2.u # es la red neuronal entrenada (sus parámetros: pesos y bias)

# ----------------------------------------------------------------------------
# C.- MAKING Prediction (UODE):
     # Generate a New Initial state vector {x₀, y₀, θ₀, κ(θ₀)}

     x₀ = [0 for i in 1:ii]
     y₀ = [0 for i in 1:ii]
     
     X_ssssol = [hcat(x₀[i], y₀[i], θ₀_true[i], κ₀_true[i])' for i in 1:ii]
    
     # Choose rod pose to test comparison
     c = 36

     # Convert the 4x1 Adjoint matrix to a Vector{Float32}
        new_initial_state = vec(X_ssssol[c])

        new_initial_state = [new_initial_state[1]; new_initial_state[2]; new_initial_state[3]; new_initial_state[4]]

    # Predict using the trained parameters
        new_prediction = Array(solve(remake(prob_nn, u0 = new_initial_state, p = p_trained), 
                                    AutoVern7(Rodas5()), saveat = s, abstol = 1e-6, reltol = 1e-6, 
                                    sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))

    # Visualizing the new UDE prediction
        # Assign new names
        px_predict = new_prediction[1,1:end]
        py_predict = new_prediction[2,1:end]
        θ_predict  = new_prediction[3,1:end]
        κ_predict  = new_prediction[4,1:end]

    # Visualizing the UODE prediction vs. Ground Truth
        # Phase plot θ(s) vs. κ(s) of the rod - UODE prediction vs. Ground Truth
        ts = first(s):mean(diff(s)):last(s)
            # θ(s)_true - UDE prediction -
                begin
                    Pp = Plots.plot(ts, vec(θ_true[c]), color=[:black :gray], label=["Training Data" nothing], legend=:best)
                    Plots.scatter!(ts, θ_predict, color=[:red :orange], label=["UODE Approximation" nothing],
                        xlabel=L"s \,\, [m]", ylabel=L"θ \,\, [rad]")
                    Plots.savefig(Pp, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_Theta-s.pdf"))
                    display(Pp)
                end
            # κ(s)_true - UDE prediction -
                
            begin
                    Pp = Plots.plot(s, vec(κ_true[c]), color=[:black :gray], label=["Training Data" nothing], legend=:best)
                    Plots.scatter!(s, κ_predict, color=[:red :orange], label=["UODE Approximation" nothing],
                        xlabel=L"s \,\, [m]", ylabel=L"\dot{\theta}\,\,=\,\,k\,\, [-]")
                    Plots.savefig(Pp, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_Kappa-s.pdf"))
                    display(Pp)
                end

            # Phase diagrams comparison
                begin
                    f44 = Plots.plot(vec(θ_true[c]), vec(κ_true[c]), color=[:red :gray], legend=:best , label="Curva original")
                    Plots.scatter!(θ_predict, κ_predict, color=[:black :gray], markersize=3, legend=:best , label="UODE",
                            xlabel=L"θ \,\, [rad]", ylabel=L"κ\,\, [-] ")
                    Plots.savefig(f44, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_phase_C_3MLP.pdf"))
                    display(f44)
                end
                
            # Error analysis θ(s), κ(s)
            θ_traj = θ_predict
            κ_traj = κ_predict
            # Global L1 & L2 error
            l1_error_θ = sum(abs.(θ_traj .- θ_true[c]))
            l2_error_θ = sqrt(sum((θ_traj .- θ_true[c]).^2))
            l1_error_κ = sum(abs.(κ_traj .- κ_true[c]))
            l2_error_κ = sqrt(sum((κ_traj .- κ_true[c]).^2))
        # Error for each point
            Errors_θ = abs.(θ_traj .- θ_true[c])
            Errors_κ = abs.(κ_traj .- κ_true[c])

    # Plot errors
        begin
            f55 = Plots.plot(1:length(Errors_θ), Errors_θ, color=:red, label=["θ error" nothing], 
                        xlabel=L"Nodos", ylabel=L"Error", title="")
            Plots.plot!(1:length(Errors_κ), Errors_κ, color=:blue, label=["κ error" nothing], 
                        xlabel=L"Nodos", ylabel=L"Error", title="")
            Plots.savefig(f55, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_theta-kappa_error.pdf"))
            display(f55)
        end

        # Shape of the rod - U=DE prediction vs. Ground Truth 
            # x(s)
                begin
                    Pp = Plots.plot(s, vec(px_true[c]), color=[:black :gray], label=["Training Data" nothing], legend=:best, aspect_ratio=:equal)
                    Plots.scatter!(s, px_predict, color=[:red :orange], label=["UODE Approximation" nothing],
                        xlabel=L"s\,\,[m]", ylabel=L"x\,\,[m]")
                    Plots.savefig(Pp, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_X-s.pdf"))
                    display(Pp)
                end
            # y(s)
            begin
                    Pp = Plots.plot(s, vec(py_true[c]), color=[:black :gray], label=["Training Data" nothing], legend=:best, aspect_ratio=:equal)
                    Plots.scatter!(s, py_predict, color=[:red :orange], label=["UODE Approximation" nothing],
                        xlabel=L"s\,\,[m]", ylabel=L"y\,\,[m]")
                    Plots.savefig(Pp, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_Y-s.pdf"))
                    display(Pp)
                end
            # (x, y)
                begin
                    f66 = Plots.plot(vec(px_true[c]), vec(py_true[c]), color=[:black :gray], label=["Curva original" nothing], legend=:best, aspect_ratio=:equal)
                    Plots.scatter!(px_predict, py_predict, color=[:blue :orange], label=["UODE" nothing],
                        xlabel=L"x\,\,[m]", ylabel=L"y\,\,[m]")
                    Plots.savefig(f66, joinpath(plots_dir, "theta_0_sol_180º_mode_2_X02_X-Y.pdf"))
                    display(f66)
                end
            # Error analysis x(s), y(s)
                x_traj = px_predict
                y_traj = py_predict

                # Global L1 & L2 error
                    l1_error_2 = sum(abs.(x_traj .- px_true[c]) .+ abs.(y_traj .- py_true[c]))
                    l2_error_2 = sqrt(sum((x_traj .- px_true[c]).^2 + (y_traj .- py_true[c]).^2))

                # L1 & L2 error for each point
                    l1_errors_2 = abs.(x_traj .- px_true[c]) .+ abs.(y_traj .- py_true[c])
                    l2_errors_2 = sqrt.((x_traj .- px_true[c]).^2 .+ (y_traj .- py_true[c]).^2)

            # Plot errors
                begin
                    #Plots.plot(1:length(l1_errors_2), l1_errors_2, color=:red, label=L"L1\,\,Error" , marker=:vline, 
                    #               xlabel=L"Nodos", ylabel=L"L1\,\,&\,\,L2\,\,\,\,Error", title="")
                    f77 = Plots.plot(1:length(l2_errors_2), l2_errors_2, color=:red, label=nothing, 
                                xlabel=L"Nodos", ylabel=L"L2\,\,\,\,Error", title="")
                    #Plots.savefig(f77, "theta_0_sol_1_mode_2_X02_X-Y_error.pdf")
                    display(f77)
                end

        # Final plots
            # θ(s), κ(s) 
                begin
                f44_55 = Plots.plot(f44, f55, layout = (1, 2), size=(1200, 500), left_margin=10Plots.mm, right_margin=10Plots.mm, top_margin=10Plots.mm, bottom_margin=10Plots.mm)
                #Plots.savefig(f44_55, "theta_0_sol_1_mode_2_X02_theta-kappa_error_T_3.4.pdf")
                display(f44_55)
                end
            # x(s), y(s)
                begin
                f66_77 = Plots.plot(f66, f77, layout = (1, 2), size=(1200, 500), left_margin=10Plots.mm, right_margin=10Plots.mm, top_margin=10Plots.mm, bottom_margin=10Plots.mm)
                #Plots.savefig(f66_77, "theta_0_sol_1_mode_2_X02_X-Y_error_3.4.pdf")
                display(f66_77)
                end
    
# -------------------------------------------------------
# D.- Save NNs

    using JLD2

    model = U
    parameters = p_trained
    states = _st

    # Save the NN data to a file using relative path for portability
    output_file = joinpath(plots_dir, "NN_NODE_1MLPx3_tanh_X02.jld2")
    println("Saving trained model to: ", output_file)

    # Create directory if it doesn't exist
    mkpath(dirname(output_file))

    @save output_file model parameters states
    println("✓ Model saved successfully to: ","$plots_dir")
# -------------------------------------------------------
```
</attachment>

</attachments>
<prompt>
Can you provide Hyperband and Bayesian optimization code for tuning the hyperparameters of the following options?

# Option 1: Pure gradient-based (fastest)
RAdam (200-500 iter) → LSQR (to convergence)

# Option 2: Gradient-free exploration (most robust)
CMA-ES (100 iter) → LSQR  (to convergence)

# Option 3: Hybrid (best of both)
DE/PSO (50 iter) → RAdam (200 iter) → LSQR  (to convergence)
</prompt>
~~~



## Response
### Assistant
~~~md
I'll provide comprehensive Hyperband and Bayesian Optimization implementations for tuning hyperparameters with LSQR as the refinement method. LSQR is particularly good for ill-conditioned problems like yours.

````julia
// filepath: c:\Users\josel\.julia\dev\ClampedPinnedRodUDE\src\solvers\UDE_HyperOpt_LSQR_Strategies.jl
# ---------------------------------------------------------------------------
# Hyperparameter Optimization for Multi-Stage UDE Training with LSQR
# 
# Implements Hyperband and Bayesian Optimization for:
# 1. RAdam → LSQR
# 2. CMA-ES → LSQR  
# 3. DE/PSO → RAdam → LSQR
# ---------------------------------------------------------------------------

# Setup project environment
include("../utils/project_utils.jl")
project_root = setup_project_environment(activate_env = true, instantiate = false)

# Create output directories
plots_dir = joinpath(project_root, "src", "Data", "NODE_HyperOpt_LSQR", "Plots_Results")
mkpath(plots_dir)
println("Results will be saved to: ", plots_dir)

# -----------------------------------------------------------------------------
# Load Packages
using Lux
using Optimisers
using LaTeXStrings
using Plots
using BenchmarkTools
gr()

# SciML Tools
using OrdinaryDiffEq, SciMLSensitivity
using Optimization, OptimizationOptimisers, OptimizationOptimJL

# For LSQR
using IterativeSolvers
using LinearAlgebra
using SparseArrays
using FiniteDiff  # For Jacobian computation

# Global optimization
using Evolutionary
using BlackBoxOptim

# Hyperparameter optimization
using Hyperopt
using BayesianOptimization
using GaussianProcesses

# Utilities
using Statistics, Random
using ComponentArrays, Lux, Zygote, StableRNGs
using DataFrames, CSV
using JSON3
using ProgressMeter

# -----------------------------------------------------------------------------
# Load your UDE setup (using code from your file)
println("Loading dataset and setting up UDE...")

using MATLAB
filename_DataSet = joinpath(project_root, "dataset", "LearnigData_Rod_Clamp_Pin_Rot_X02_Y00_mode2_train_085.mat")
println("Loading dataset from: ", filename_DataSet)

if !isfile(filename_DataSet)
    error("Dataset file not found at: $filename_DataSet")
end

mf_DataSet = MatFile(filename_DataSet)
data_DataSet = get_mvariable(mf_DataSet, "DataSet_train")
DataSet = jarray(data_DataSet)

# Dynamically determine the number of trajectories
num_trajectories = size(DataSet, 1)
println("Found $num_trajectories trajectories in the dataset")

# Use subset for hyperopt speed
max_trajectories_to_use = 10  # Adjust based on computational budget
trajectories_to_use = min(max_trajectories_to_use, num_trajectories)
println("Using $trajectories_to_use out of $num_trajectories available trajectories")

# Initialize variables
θ₀_true, κ₀_true = [], []
px_true, py_true, θ_true, κ_true = Vector{Float32}[], Vector{Float32}[], Vector{Float32}[], Vector{Float32}[]

# Load data
for i in 1:trajectories_to_use
    θ₀_i  = Float32.(DataSet[i,3])
    κ₀_i  = Float32.(DataSet[i,112])
    push!(θ₀_true,  θ₀_i)
    push!(κ₀_true,  κ₀_i)
end
θ₀_true = Float32.(collect(θ₀_true))
κ₀_true = Float32.(collect(κ₀_true))

ii = trajectories_to_use
for i in 1:ii
    pX_i = Float32.(DataSet[i,12:61])
    pY_i = Float32.(DataSet[i,62:111])
    θ_i  = Float32.(DataSet[i,162:211])
    κ_i  = Float32.(DataSet[i,112:161])
    push!(px_true, pX_i)
    push!(py_true, pY_i)
    push!(θ_true,  θ_i)
    push!(κ_true,  κ_i)
end

# Combine all trajectories into a single Learning dataset
X_ssol_all = [hcat(px_true[i], py_true[i], θ_true[i], κ_true[i])' for i in 1:ii]

# Setup neural network
rng = StableRNG(1111)
m = 4
n = 3
layers = collect([m, 20, 20, 20, n])

# Multilayer FeedForward
const U = Lux.Chain(
    [Dense(fan_in => fan_out, Lux.tanh) for (fan_in, fan_out) in zip(layers[1:end-2], layers[2:end-1])]...,
    Dense(layers[end-1] => layers[end], identity),
) 

# Get the initial parameters and state variables
p_init, st = Lux.setup(rng, U)
p_init = ComponentArray{Float32}(p_init)
const _st = st

# Global counters for NN passes
const NN_FORWARD_COUNT = Ref(0)
const NN_BACKWARD_COUNT = Ref(0)

# Define the ODE
function ude_dynamics!(du, u, p, s)
    NN_FORWARD_COUNT[] += 1
    if any(!isfinite, u)
        du .= 0.0f0
        return nothing
    end
    û_1 = U(u, p, _st)[1] 
    
    du[1] = û_1[1]   # dx/ds
    du[2] = û_1[2]   # dy/ds
    du[3] = u[4]     # dθ/ds = κ
    du[4] = û_1[3]   # dκ/ds
    return nothing
end 

# Sampling & model parameter space
sSpan = (0.0f0, 1.0f0)
s = Float32.(range(0, 1, length=50))

# Use the first trajectory's initial state to define prob_nn
u0 = Float32.(X_ssol_all[1][:, 1])
const prob_nn = ODEProblem(ude_dynamics!, u0, sSpan, p_init)

# Prediction function
function predict(θ, X_all = X_ssol_all, S = s)
    try
        [Array(solve(remake(prob_nn, u0 = Float32.(X[:, 1]), tspan = (S[1], S[end]), p = θ),    
                AutoVern7(Rodas5()), saveat = S, abstol = 1e-6, reltol = 1e-6,                  
                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))) for X in X_all]
    catch e
        return nothing
    end
end

# Loss function
function loss(θ)
    X̂_sols = predict(θ)
    if X̂_sols === nothing
        return 1e10
    end
    total_loss = sum(mean(abs, X_ssol_all[i] .- X̂_sols[i]) for i in 1:length(X̂_sols))
    return total_loss / length(X̂_sols)
end

# -----------------------------------------------------------------------------
# LSQR IMPLEMENTATION FOR REFINEMENT

"""
    compute_residuals_and_jacobian(θ)
    
Compute residuals and Jacobian for LSQR method
"""
function compute_residuals_and_jacobian(θ; use_finite_diff=true)
    # Compute current predictions
    X̂_sols = predict(θ)
    if X̂_sols === nothing
        return nothing, nothing
    end
    
    # Flatten residuals
    residuals = Float64[]
    for i in 1:length(X̂_sols)
        append!(residuals, vec(X_ssol_all[i] .- X̂_sols[i]))
    end
    
    if use_finite_diff
        # Use finite differences for Jacobian (more stable for ill-conditioned problems)
        residual_func = θ_vec -> begin
            θ_comp = ComponentArray{Float32}(θ_vec, axes(θ))
            X̂_sols_j = predict(θ_comp)
            if X̂_sols_j === nothing
                return fill(1e10, length(residuals))
            end
            res = Float64[]
            for i in 1:length(X̂_sols_j)
                append!(res, vec(X_ssol_all[i] .- X̂_sols_j[i]))
            end
            return res
        end
        
        # Compute Jacobian using central differences
        J = FiniteDiff.finite_difference_jacobian(residual_func, vec(θ))
    else
        # Use automatic differentiation (if preferred)
        # Note: This might be less stable for ill-conditioned problems
        J = Zygote.jacobian(θ -> begin
            X̂_sols_j = predict(θ)
            if X̂_sols_j === nothing
                return fill(1e10, length(residuals))
            end
            res = Float64[]
            for i in 1:length(X̂_sols_j)
                append!(res, vec(X_ssol_all[i] .- X̂_sols_j[i]))
            end
            return res
        end, θ)[1]
    end
    
    return residuals, J
end

"""
    lsqr_refinement(θ_init; hyperparams)
    
Refine parameters using LSQR method
"""
function lsqr_refinement(θ_init; hyperparams=Dict())
    # Extract hyperparameters with defaults
    max_iters = get(hyperparams, :lsqr_maxiter, 100)
    atol = get(hyperparams, :lsqr_atol, 1e-8)
    btol = get(hyperparams, :lsqr_btol, 1e-8)
    λ_reg = get(hyperparams, :lsqr_lambda, 1e-6)
    max_outer_iters = get(hyperparams, :lsqr_outer_iters, 20)
    line_search_alpha = get(hyperparams, :lsqr_alpha, 0.5)
    
    θ_current = copy(θ_init)
    losses_lsqr = Float64[]
    
    for outer_iter in 1:max_outer_iters
        # Compute residuals and Jacobian
        residuals, J = compute_residuals_and_jacobian(θ_current)
        
        if residuals === nothing || J === nothing
            println("  LSQR: Failed to compute residuals/Jacobian at iteration $outer_iter")
            break
        end
        
        current_loss = norm(residuals)^2 / length(residuals)
        push!(losses_lsqr, current_loss)
        
        # Solve using LSQR
        Δθ, stats = lsqr(J, residuals,
                         atol=atol,
                         btol=btol,
                         maxiter=max_iters,
                         λ=λ_reg,
                         verbose=false)
        
        # Line search for stable update
        α = line_search_alpha
        improvement_found = false
        
        for _ in 1:10
            θ_test = θ_current - α * ComponentArray{Float32}(Δθ, axes(θ_current))
            test_loss = loss(θ_test)
            
            if test_loss < current_loss
                θ_current = θ_test
                improvement_found = true
                break
            end
            α *= 0.5
        end
        
        if !improvement_found
            println("  LSQR: No improvement found at iteration $outer_iter, stopping")
            break
        end
        
        # Check convergence
        if outer_iter > 1 && abs(losses_lsqr[end] - losses_lsqr[end-1]) < 1e-6
            println("  LSQR: Converged at iteration $outer_iter")
            break
        end
    end
    
    return θ_current, losses_lsqr
end

# -----------------------------------------------------------------------------
# STRATEGY IMPLEMENTATIONS WITH LSQR

"""
    train_strategy_1(hyperparams; max_iters_dict)
    
Option 1: RAdam → LSQR
"""
function train_strategy_1(hyperparams; max_iters_dict=Dict(:radam=>200, :lsqr=>20))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: RAdam
    println("  Strategy 1 - Phase 1: RAdam")
    losses_radam = Float64[]
    callback = function (p, l)
        push!(losses_radam, l)
        push!(all_losses, l)
        return false
    end
    
    adtype = Optimization.AutoZygote()
    optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_current)
    
    t1 = @elapsed begin
        res_radam = Optimization.solve(optprob,
                                       Optimisers.RAdam(hyperparams[:radam_lr]),
                                       callback = callback,
                                       maxiters = hyperparams[:radam_iters])
    end
    total_time += t1
    p_current = res_radam.u
    
    # Phase 2: LSQR
    println("  Strategy 1 - Phase 2: LSQR")
    
    lsqr_hyperparams = Dict(
        :lsqr_maxiter => hyperparams[:lsqr_maxiter],
        :lsqr_atol => hyperparams[:lsqr_atol],
        :lsqr_btol => hyperparams[:lsqr_btol],
        :lsqr_lambda => hyperparams[:lsqr_lambda],
        :lsqr_outer_iters => max_iters_dict[:lsqr],
        :lsqr_alpha => hyperparams[:lsqr_alpha]
    )
    
    t2 = @elapsed begin
        p_current, losses_lsqr = lsqr_refinement(p_current; hyperparams=lsqr_hyperparams)
        append!(all_losses, losses_lsqr)
    end
    total_time += t2
    
    final_loss = loss(p_current)
    
    return final_loss, p_current, all_losses, total_time
end

"""
    train_strategy_2(hyperparams; max_iters_dict)
    
Option 2: CMA-ES → LSQR
"""
function train_strategy_2(hyperparams; max_iters_dict=Dict(:cmaes=>100, :lsqr=>20))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: CMA-ES
    println("  Strategy 2 - Phase 1: CMA-ES")
    
    function loss_vec(θ_vec)
        θ_comp = ComponentArray{Float32}(θ_vec, axes(p_current))
        return loss(θ_comp)
    end
    
    lower_bounds = fill(-5.0, length(vec(p_current)))
    upper_bounds = fill(5.0, length(vec(p_current)))
    
    n_params = length(vec(p_current))
    if hyperparams[:cmaes_popsize] == "auto"
        μ = div(n_params, 4)
        λ = 2 * μ
    else
        λ = hyperparams[:cmaes_popsize]
        μ = div(λ, 2)
    end
    
    t1 = @elapsed begin
        result_cmaes = Evolutionary.optimize(
            loss_vec,
            BoxConstraints(lower_bounds, upper_bounds),
            CMAES(μ = μ, λ = λ),
            Evolutionary.Options(
                iterations = hyperparams[:cmaes_iters],
                show_trace = false,
                store_trace = true
            )
        )
        p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_cmaes), axes(p_current))
        
        # Extract losses from trace
        for state in result_cmaes.trace
            push!(all_losses, state.value)
        end
    end
    total_time += t1
    
    # Phase 2: LSQR
    println("  Strategy 2 - Phase 2: LSQR")
    
    lsqr_hyperparams = Dict(
        :lsqr_maxiter => hyperparams[:lsqr_maxiter],
        :lsqr_atol => hyperparams[:lsqr_atol],
        :lsqr_btol => hyperparams[:lsqr_btol],
        :lsqr_lambda => hyperparams[:lsqr_lambda],
        :lsqr_outer_iters => max_iters_dict[:lsqr],
        :lsqr_alpha => hyperparams[:lsqr_alpha]
    )
    
    t2 = @elapsed begin
        p_current, losses_lsqr = lsqr_refinement(p_current; hyperparams=lsqr_hyperparams)
        append!(all_losses, losses_lsqr)
    end
    total_time += t2
    
    final_loss = loss(p_current)
    
    return final_loss, p_current, all_losses, total_time
end

"""
    train_strategy_3(hyperparams; max_iters_dict)
    
Option 3: DE/PSO → RAdam → LSQR
"""
function train_strategy_3(hyperparams; max_iters_dict=Dict(:global=>50, :radam=>200, :lsqr=>20))
    p_current = copy(p_init)
    total_time = 0.0
    all_losses = Float64[]
    
    # Phase 1: DE or PSO
    function loss_vec(θ_vec)
        θ_comp = ComponentArray{Float32}(θ_vec, axes(p_current))
        return loss(θ_comp)
    end
    
    lower_bounds = fill(-5.0, length(vec(p_current)))
    upper_bounds = fill(5.0, length(vec(p_current)))
    
    if hyperparams[:use_de]
        println("  Strategy 3 - Phase 1: Differential Evolution")
        t1 = @elapsed begin
            result_de = Evolutionary.optimize(
                loss_vec,
                BoxConstraints(lower_bounds, upper_bounds),
                DE(populationSize = hyperparams[:de_popsize],
                   F = hyperparams[:de_f],
                   CR = hyperparams[:de_cr]),
                Evolutionary.Options(
                    iterations = hyperparams[:global_iters],
                    show_trace = false,
                    store_trace = true
                )
            )
            p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_de), axes(p_current))
            for state in result_de.trace
                push!(all_losses, state.value)
            end
        end
    else
        println("  Strategy 3 - Phase 1: Particle Swarm")
        t1 = @elapsed begin
            result_pso = Evolutionary.optimize(
                loss_vec,
                BoxConstraints(lower_bounds, upper_bounds),
                PSO(n_particles = hyperparams[:pso_particles],
                    w = hyperparams[:pso_w],
                    c1 = hyperparams[:pso_c1],
                    c2 = hyperparams[:pso_c2]),
                Evolutionary.Options(
                    iterations = hyperparams[:global_iters],
                    show_trace = false,
                    store_trace = true
                )
            )
            p_current = ComponentArray{Float32}(Evolutionary.minimizer(result_pso), axes(p_current))
            for state in result_pso.trace
                push!(all_losses, state.value)
            end
        end
    end
    total_time += t1
    
    # Phase 2: RAdam
    println("  Strategy 3 - Phase 2: RAdam")
    adtype = Optimization.AutoZygote()
    optf = Optimization.OptimizationFunction((x, q) -> loss(x), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_current)
    
    losses_radam = Float64[]
    callback = function (p, l)
        push!(losses_radam, l)
        push!(all_losses, l)
        return false
    end
    
    t2 = @elapsed begin
        res_radam = Optimization.solve(optprob,
                                       Optimisers.RAdam(hyperparams[:radam_lr]),
                                       callback = callback,
                                       maxiters = hyperparams[:radam_iters])
    end
    total_time += t2
    p_current = res_radam.u
    
    # Phase 3: LSQR
    println("  Strategy 3 - Phase 3: LSQR")
    
    lsqr_hyperparams = Dict(
        :lsqr_maxiter => hyperparams[:lsqr_maxiter],
        :lsqr_atol => hyperparams[:lsqr_atol],
        :lsqr_btol => hyperparams[:lsqr_btol],
        :lsqr_lambda => hyperparams[:lsqr_lambda],
        :lsqr_outer_iters => max_iters_dict[:lsqr],
        :lsqr_alpha => hyperparams[:lsqr_alpha]
    )
    
    t3 = @elapsed begin
        p_current, losses_lsqr = lsqr_refinement(p_current; hyperparams=lsqr_hyperparams)
        append!(all_losses, losses_lsqr)
    end
    total_time += t3
    
    final_loss = loss(p_current)
    
    return final_loss, p_current, all_losses, total_time
end

# -----------------------------------------------------------------------------
# HYPERBAND IMPLEMENTATION

println("\n" * "="^60)
println("HYPERBAND OPTIMIZATION WITH LSQR")
println("="^60)

function hyperband_for_strategy(strategy_fn, hyperspace, strategy_name; 
                                max_resource=400, η=3)
    """
    Hyperband for a given strategy with LSQR
    """
    
    R = max_resource
    s_max = floor(Int, log(R) / log(η))
    B = (s_max + 1) * R
    
    best_config = nothing
    best_loss = Inf
    all_results = []
    
    println("Running Hyperband for $strategy_name")
    println("s_max = $s_max, B = $B")
    
    for s in s_max:-1:0
        println("\nBracket s = $s")
        n = ceil(Int, B / R / (s + 1) * η^s / (η^s))
        r = R * η^(-s)
        
        println("Initial configs: n = $n, initial resource: r = $r")
        
        # Generate n random configurations
        configs = []
        for i in 1:n
            config = Dict{Symbol, Any}()
            for (param_name, param_range) in hyperspace
                if param_range isa Tuple{Float64, Float64}
                    config[param_name] = param_range[1] + (param_range[2] - param_range[1]) * rand()
                elseif param_range isa Tuple{Int, Int}
                    config[param_name] = rand(param_range[1]:param_range[2])
                elseif param_range isa Vector
                    config[param_name] = rand(param_range)
                elseif param_range isa Tuple{String, String} && param_range[1] == "log"
                    low, high = log(parse(Float64, split(param_range[2], ":")[1])), 
                               log(parse(Float64, split(param_range[2], ":")[2]))
                    config[param_name] = exp(low + (high - low) * rand())
                end
            end
            push!(configs, config)
        end
        
        # Successive halving
        for i in 0:s
            n_i = floor(Int, n * η^(-i))
            r_i = floor(Int, r * η^i)
            
            println("  Round $i: evaluating $(min(n_i, length(configs))) configs with resource $r_i")
            
            # Evaluate configurations
            results = []
            for (idx, config) in enumerate(configs[1:min(n_i, length(configs))])
                # Adjust iterations based on resource
                if haskey(config, :radam_iters)
                    config[:radam_iters] = min(config[:radam_iters], r_i)
                end
                if haskey(config, :cmaes_iters)
                    config[:cmaes_iters] = min(config[:cmaes_iters], r_i)
                end
                if haskey(config, :global_iters)
                    config[:global_iters] = min(config[:global_iters], div(r_i, 3))
                    config[:radam_iters] = min(config[:radam_iters], div(r_i, 2))
                end
                
                try
                    loss_val, _, _, time = strategy_fn(config)
                    push!(results, (config=config, loss=loss_val, time=time))
                    println("    Config $idx: loss = $(round(loss_val, digits=6))")
                catch e
                    println("    Config $idx failed: $e")
                    push!(results, (config=config, loss=1e10, time=0.0))
                end
            end
            
            # Keep best configurations
            sort!(results, by=x->x.loss)
            n_keep = max(1, floor(Int, n_i/η))
            configs = [r.config for r in results[1:min(n_keep, length(results))]]
            
            # Track best
            if length(results) > 0 && results[1].loss < best_loss
                best_loss = results[1].loss
                best_config = results[1].config
            end
            
            push!(all_results, results)
        end
    end
    
    return best_config, best_loss, all_results
end

# Define hyperparameter spaces for LSQR-based strategies
hyperspace_strategy1 = [
    :radam_lr => ("log", "1e-5:1e-1"),
    :radam_iters => (100, 500),
    :lsqr_maxiter => (50, 200),
    :lsqr_atol => ("log", "1e-10:1e-6"),
    :lsqr_btol => ("log", "1e-10:1e-6"),
    :lsqr_lambda => ("log", "1e-8:1e-3"),
    :lsqr_alpha => (0.1, 1.0)
]

hyperspace_strategy2 = [
    :cmaes_popsize => ["auto", 20, 50, 100],
    :cmaes_iters => (50, 200),
    :lsqr_maxiter => (50, 200),
    :lsqr_atol => ("log", "1e-10:1e-6"),
    :lsqr_btol => ("log", "1e-10:1e-6"),
    :lsqr_lambda => ("log", "1e-8:1e-3"),
    :lsqr_alpha => (0.1, 1.0)
]

hyperspace_strategy3 = [
    :use_de => [true, false],
    :de_popsize => (20, 100),
    :de_f => (0.5, 1.0),
    :de_cr => (0.7, 1.0),
    :pso_particles => (20, 50),
    :pso_w => (0.4, 0.9),
    :pso_c1 => (1.0, 2.0),
    :pso_c2 => (1.0, 2.0),
    :global_iters => (30, 100),
    :radam_lr => ("log", "1e-5:1e-1"),
    :radam_iters => (100, 300),
    :lsqr_maxiter => (50, 200),
    :lsqr_atol => ("log", "1e-10:1e-6"),
    :lsqr_btol => ("log", "1e-10:1e-6"),
    :lsqr_lambda => ("log", "1e-8:1e-3"),
    :lsqr_alpha => (0.1, 1.0)
]

# Run Hyperband for each strategy
hyperband_results = Dict()

println("\n" * "="^30 * " STRATEGY 1 " * "="^30)
best_config1, best_loss1, results1 = hyperband_for_strategy(
    train_strategy_1, hyperspace_strategy1, "Strategy 1: RAdam → LSQR",
    max_resource=300, η=3
)
hyperband_results["Strategy1"] = (config=best_config1, loss=best_loss1)

println("\n" * "="^30 * " STRATEGY 2 " * "="^30)
best_config2, best_loss2, results2 = hyperband_for_strategy(
    train_strategy_2, hyperspace_strategy2, "Strategy 2: CMA-ES → LSQR",
    max_resource=250, η=3
)
hyperband_results["Strategy2"] = (config=best_config2, loss=best_loss2)

println("\n" * "="^30 * " STRATEGY 3 " * "="^30)
best_config3, best_loss3, results3 = hyperband_for_strategy(
    train_strategy_3, hyperspace_strategy3, "Strategy 3: DE/PSO → RAdam → LSQR",
    max_resource=400, η=3
)
hyperband_results["Strategy3"] = (config=best_config3, loss=best_loss3)

# -----------------------------------------------------------------------------
# BAYESIAN OPTIMIZATION IMPLEMENTATION

println("\n" * "="^60)
println("BAYESIAN OPTIMIZATION WITH LSQR")
println("="^60)

using GaussianProcesses
using Optim

function bayesian_optimization_strategy(strategy_fn, bounds_dict, strategy_name;
                                       n_initial=5, n_iterations=20)
    
    println("Running Bayesian Optimization for $strategy_name")
    
    # Convert bounds to arrays
    param_names = collect(keys(bounds_dict))
    lower_bounds = Float64[]
    upper_bounds = Float64[]
    log_scale = Bool[]
    
    for (param, bound) in bounds_dict
        if bound[1] == "log"
            push!(lower_bounds, log(bound[2]))
            push!(upper_bounds, log(bound[3]))
            push!(log_scale, true)
        else
            push!(lower_bounds, bound[1])
            push!(upper_bounds, bound[2])
            push!(log_scale, false)
        end
    end
    
    n_dims = length(param_names)
    
    # Initial random sampling
    X_observed = zeros(n_dims, 0)
    y_observed = Float64[]
    
    println("Initial random sampling ($n_initial points)...")
    for i in 1:n_initial
        # Sample random point
        x = zeros(n_dims)
        for j in 1:n_dims
            x[j] = lower_bounds[j] + (upper_bounds[j] - lower_bounds[j]) * rand()
        end
        
        # Convert to hyperparameters
        config = Dict{Symbol, Any}()
        for (j, param) in enumerate(param_names)
            if log_scale[j]
                config[param] = exp(x[j])
            else
                config[param] = x[j]
            end
            
            # Type conversions
            if param in [:radam_iters, :cmaes_iters, :global_iters, :lsqr_maxiter,
                        :de_popsize, :pso_particles]
                config[param] = round(Int, config[param])
            end
            if param == :use_de
                config[param] = config[param] > 0.5
            end
        end
        
        # Evaluate
        try
            loss_val, _, _, _ = strategy_fn(config)
            X_observed = hcat(X_observed, x)
            push!(y_observed, loss_val)
            println("  Sample $i: loss = $(round(loss_val, digits=6))")
        catch e
            println("  Sample $i failed: $e")
            X_observed = hcat(X_observed, x)
            push!(y_observed, 1e10)
        end
    end
    
    # Bayesian optimization loop
    for iter in 1:n_iterations
        println("\nBO Iteration $iter/$n_iterations")
        
        # Fit Gaussian Process
        y_mean = mean(y_observed)
        y_std = std(y_observed) + 1e-6
        y_normalized = (y_observed .- y_mean) ./ y_std
        
        # Create GP with Matern kernel
        kern = Mat52Iso(1.0, 1.0)
        gp = GP(X_observed', y_normalized, MeanConst(0.0), kern, -2.0)
        
        # Optimize GP hyperparameters
        optimize!(gp)
        
        # Acquisition function: Expected Improvement
        best_y = minimum(y_normalized)
        
        function acquisition(x)
            μ, σ² = predict_f(gp, reshape(x, :, 1))
            σ = sqrt(σ²[1] + 1e-6)
            
            # Expected Improvement
            if σ > 0
                Z = (best_y - μ[1]) / σ
                Φ = cdf(Normal(), Z)
                φ = pdf(Normal(), Z)
                ei = σ * (Z * Φ + φ)
                return -ei  # Minimize negative EI
            else
                return 0.0
            end
        end
        
        # Find next point
        result = optimize(acquisition, lower_bounds, upper_bounds, 
                         lower_bounds .+ (upper_bounds .- lower_bounds) .* rand(n_dims),
                         Fminbox(LBFGS()), 
                         Optim.Options(iterations=100))
        x_next = Optim.minimizer(result)
        
        # Convert and evaluate
        config = Dict{Symbol, Any}()
        for (j, param) in enumerate(param_names)
            if log_scale[j]
                config[param] = exp(x_next[j])
            else
                config[param] = x_next[j]
            end
            
            # Type conversions
            if param in [:radam_iters, :cmaes_iters, :global_iters, :lsqr_maxiter,
                        :de_popsize, :pso_particles]
                config[param] = round(Int, config[param])
            end
            if param == :use_de
                config[param] = config[param] > 0.5
            end
        end
        
        # Evaluate
        try
            loss_val, _, _, _ = strategy_fn(config)
            X_observed = hcat(X_observed, x_next)
            push!(y_observed, loss_val)
            println("  New sample: loss = $(round(loss_val, digits=6))")
            
            if loss_val < minimum(y_observed[1:end-1])
                println("  🎯 New best configuration found!")
            end
        catch e
            println("  Evaluation failed: $e")
            X_observed = hcat(X_observed, x_next)
            push!(y_observed, 1e10)
        end
    end
    
    # Return best configuration
    best_idx = argmin(y_observed)
    best_x = X_observed[:, best_idx]
    
    best_config = Dict{Symbol, Any}()
    for (j, param) in enumerate(param_names)
        if log_scale[j]
            best_config[param] = exp(best_x[j])
        else
            best_config[param] = best_x[j]
        end
        
        # Type conversions
        if param in [:radam_iters, :cmaes_iters, :global_iters, :lsqr_maxiter,
                    :de_popsize, :pso_particles]
            best_config[param] = round(Int, best_config[param])
        end
        if param == :use_de
            best_config[param] = best_config[param] > 0.5
        end
    end
    
    return best_config, y_observed[best_idx], X_observed, y_observed
end

# Define bounds for Bayesian Optimization with LSQR
bo_bounds_strategy1 = Dict(
    :radam_lr => ("log", 1e-5, 1e-1),
    :radam_iters => ("linear", 100.0, 500.0),
    :lsqr_maxiter => ("linear", 50.0, 200.0),
    :lsqr_atol => ("log", 1e-10, 1e-6),
    :lsqr_btol => ("log", 1e-10, 1e-6),
    :lsqr_lambda => ("log", 1e-8, 1e-3),
    :lsqr_alpha => ("linear", 0.1, 1.0)
)

bo_bounds_strategy2 = Dict(
    :cmaes_popsize => ("linear", 20.0, 100.0),
    :cmaes_iters => ("linear", 50.0, 200.0),
    :lsqr_maxiter => ("linear", 50.0, 200.0),
    :lsqr_atol => ("log", 1e-10, 1e-6),
    :lsqr_btol => ("log", 1e-10, 1e-6),
    :lsqr_lambda => ("log", 1e-8, 1e-3),
    :lsqr_alpha => ("linear", 0.1, 1.0)
)

bo_bounds_strategy3 = Dict(
    :use_de => ("linear", 0.0, 1.0),
    :de_popsize => ("linear", 20.0, 100.0),
    :de_f => ("linear", 0.5, 1.0),
    :de_cr => ("linear", 0.7, 1.0),
    :pso_particles => ("linear", 20.0, 50.0),
    :pso_w => ("linear", 0.4, 0.9),
    :global_iters => ("linear", 30.0, 100.0),
    :radam_lr => ("log", 1e-5, 1e-1),
    :radam_iters => ("linear", 100.0, 300.0),
    :lsqr_maxiter => ("linear", 50.0, 200.0),
    :lsqr_atol => ("log", 1e-10, 1e-6),
    :lsqr_btol => ("log", 1e-10, 1e-6),
    :lsqr_lambda => ("log", 1e-8, 1e-3),
    :lsqr_alpha => ("linear", 0.1, 1.0)
)

# Run Bayesian Optimization
bo_results = Dict()

println("\n" * "="^30 * " BO STRATEGY 1 " * "="^30)
bo_config1, bo_loss1, _, _ = bayesian_optimization_strategy(
    train_strategy_1, bo_bounds_strategy1, 
    "Strategy 1: RAdam → LSQR",
    n_initial=5, n_iterations=15
)
bo_results["Strategy1"] = (config=bo_config1, loss=bo_loss1)

println("\n" * "="^30 * " BO STRATEGY 2 " * "="^30)
bo_config2, bo_loss2, _, _ = bayesian_optimization_strategy(
    train_strategy_2, bo_bounds_strategy2,
    "Strategy 2: CMA-ES → LSQR",
    n_initial=5, n_iterations=15
)
bo_results["Strategy2"] = (config=bo_config2, loss=bo_loss2)

println("\n" * "="^30 * " BO STRATEGY 3 " * "="^30)
bo_config3, bo_loss3, _, _ = bayesian_optimization_strategy(
    train_strategy_3, bo_bounds_strategy3,
    "Strategy 3: DE/PSO → RAdam → LSQR",
    n_initial=5, n_iterations=15
)
bo_results["Strategy3"] = (config=bo_config3, loss=bo_loss3)

# -----------------------------------------------------------------------------
# FINAL COMPARISON

println("\n" * "="^60)
println("FINAL RESULTS - LSQR STRATEGIES")
println("="^60)

# Create comparison table
comparison_df = DataFrame(
    Strategy = String[],
    Method = String[],
    BestLoss = Float64[],
    Time = String[]
)

for (strategy_name, result) in hyperband_results
    push!(comparison_df, (
        strategy_name, 
        "Hyperband", 
        result.loss,
        "N/A"
    ))
end

for (strategy_name, result) in bo_results
    push!(comparison_df, (
        strategy_name, 
        "Bayesian Opt", 
        result.loss,
        "N/A"
    ))
end

sort!(comparison_df, :BestLoss)
println(comparison_df)

# Save results
best_configs = Dict(
    "Hyperband" => hyperband_results,
    "BayesianOpt" => bo_results,
    "Comparison" => comparison_df
)

open(joinpath(plots_dir, "best_hyperparameters_lsqr.json"), "w") do io
    JSON3.write(io, best_configs)
end

# Print best overall configuration
overall_best = comparison_df[1, :]
println("\n" * "="^60)
println("🏆 BEST CONFIGURATION (LSQR-based)")
println("="^60)
println("Strategy: $(overall_best.Strategy)")
println("Method: $(overall_best.Method)")
println("Loss: $(overall_best.BestLoss)")

# Retrieve and display best configuration details
best_strategy = overall_best.Strategy
best_method = overall_best.Method

if best_method == "Hyperband"
    best_config = hyperband_results[best_strategy].config
else
    best_config = bo_results[best_strategy].config
end

println("\nBest Hyperparameters:")
for (key, value) in best_config
    println("  $key: $value")
end

println("="^60)
println("\n✅ Hyperparameter optimization with LSQR complete!")
println("Results saved to: $plots_dir")

# Plot comparison
pl_comparison = plot(
    title="LSQR-based Strategy Optimization",
    xlabel="Strategy",
    ylabel="Best Loss",
    yaxis=:log10,
    legend=:topright
)

strategies = ["Strategy1", "Strategy2", "Strategy3"]
hb_losses = [hyperband_results[s].loss for s in strategies]
bo_losses = [bo_results[s].loss for s in strategies]

bar!(pl_comparison, strategies, hb_losses, label="Hyperband", alpha=0.7, color=:blue)
bar!(pl_comparison, strategies, bo_losses, label="Bayesian Opt", alpha=0.7, color=:red)

savefig(pl_comparison, joinpath(plots_dir, "lsqr_strategies_comparison.pdf"))
display(pl_comparison)
````

This implementation provides comprehensive hyperparameter optimization for LSQR-based strategies with the following key features:

## **Key Features:**

1. **LSQR Implementation:**
   - Iterative solver for ill-conditioned problems
   - Automatic regularization via λ parameter
   - Line search for stable updates
   - Convergence monitoring

2. **Three Optimized Strategies:**
   - **Strategy 1**: RAdam → LSQR (fastest convergence)
   - **Strategy 2**: CMA-ES → LSQR (most robust)
   - **Strategy 3**: DE/PSO → RAdam → LSQR (best exploration)

3. **LSQR-specific Hyperparameters:**
   - `lsqr_maxiter`: Inner LSQR iterations
   - `lsqr_atol`, `lsqr_btol`: Convergence tolerances
   - `lsqr_lambda`: Regularization parameter
   - `lsqr_alpha`: Line search step size
   - `lsqr_outer_iters`: Outer iteration loops

4. **Advantages of LSQR over LBFGS/LM:**
   - Better handling of ill-conditioned Jacobians
   - Built-in regularization
   - Memory efficient for large problems
   - Automatic stopping criteria

The code automatically finds the best hyperparameters for each strategy and saves the results for production use.
~~~


<style>
[id^="system"], [id^="user"], [id^="assistant"] {
		margin: 4px 0 4px 0;
}

.markdown-body > pre {
		padding: 4px 16px;
}
</style>
